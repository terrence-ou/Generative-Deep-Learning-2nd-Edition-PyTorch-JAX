{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e1ee6e-4367-4ef1-b15c-9ad225053267",
   "metadata": {},
   "source": [
    "# LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e0b4f-1228-4eda-8842-9bf19873b034",
   "metadata": {},
   "source": [
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/01_lstm/lstm.ipynb)\n",
    "- Dataset: [Kaggle](https://www.kaggle.com/datasets/hugodarwood/epirecipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d625ea0-486e-4619-ad9d-0ebbd7c24d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "from clu import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76dd9c-8a94-4b81-907d-b0a64ee0c905",
   "metadata": {},
   "source": [
    "## 0. Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4684fb-c1d3-41b8-b7da-4ea8da5dfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/epirecipes/full_format_recipes.json'\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LSTM_LAYERS = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "VOCAB_SIZE = 8200\n",
    "LR = 1e-3\n",
    "\n",
    "MAX_PAD_LEN = 200\n",
    "MAX_VAL_TOKENS = 100 # Max number of tokens when generating texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7785935-9cdb-4b7f-87d4-c62df452fa08",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c07bf4e-9e45-4811-a65c-b0e5378aeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(sentence):\n",
    "    sentence = re.sub(f'([{string.punctuation}])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad38cac-1ae6-42f7-95ef-3a9bff45e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open(DATA_DIR, 'r+') as f:\n",
    "    recipe_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f19bd7-9d03-4f72-8191-7d082c22663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recipe loaded: 20098\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "filtered_data = [\n",
    "    'Recipe for ' + x['title'] + ' | ' + ' '.join(x['directions'])\n",
    "    for x in recipe_data\n",
    "    if 'title' in x and x['title']\n",
    "    and 'directions' in x and x['directions']\n",
    "]\n",
    "\n",
    "text_ds = [pad_punctuation(sentence) for sentence in filtered_data]\n",
    "print(f'Total recipe loaded: {len(text_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0bc4013-f1cb-46ea-81d1-abc8017365dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "Recipe for Buttermilk Spoon Bread | Preheat oven to 350°F . Generously butter 13x9x2 - inch glass baking dish . Whisk eggs and cream in large bowl to blend . Bring 4 cups water , buttermilk , butter , salt , and pepper to boil in heavy large saucepan over medium - high heat . Gradually whisk in cornmeal . Reduce heat to medium - low . Simmer until thick and smooth , whisking frequently , about 4 minutes . Gradually whisk hot cornmeal into egg mixture . Transfer batter to prepared dish . Bake uncovered until golden on top and gently set in center , about 30 minutes . Let stand 10 minutes . Serve warm , directly from dish . \n"
     ]
    }
   ],
   "source": [
    "print('Sample data:')\n",
    "sample_data = np.random.choice(text_ds)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2809b15-a4bc-488c-a8cd-ae70f5fd1dae",
   "metadata": {},
   "source": [
    "## 2. Build vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8cf15a-6a3c-4b16-a7f5-06426b294859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver texts list to tf dataset\n",
    "text_ds_tf = Dataset.from_tensor_slices(text_ds)\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_PAD_LEN+1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ccea64-db03-4e3b-b30c-6c2e4c88f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: .\n",
      "3: ,\n",
      "4: and\n",
      "5: to\n",
      "6: in\n",
      "7: the\n",
      "8: with\n",
      "9: a\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds_tf)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "index_to_word = {index : word for index, word in enumerate(vocab)}\n",
    "word_to_index = {word : index for index, word in enumerate(vocab)}\n",
    "\n",
    "# First 10 items in the vocabulary\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f'{i}: {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c385672-4086-4ddb-8113-e9efd56c2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text:\n",
      "Recipe for Buttermilk Spoon Bread | Preheat oven to 350°F . Generously butter 13x9x2 - inch glass baking dish . Whisk eggs and cream in large bowl to blend . Bring 4 cups water , buttermilk , butter , salt , and pepper to boil in heavy large saucepan over medium - high heat . Gradually whisk in cornmeal . Reduce heat to medium - low . Simmer until thick and smooth , whisking frequently , about 4 minutes . Gradually whisk hot cornmeal into egg mixture . Transfer batter to prepared dish . Bake uncovered until golden on top and gently set in center , about 30 minutes . Let stand 10 minutes . Serve warm , directly from dish . \n",
      "\n",
      "\n",
      "Mapped sample:\n",
      "[  26   16  599   98  195   27   86   47    5  216    2  540   50 1010\n",
      "   13   53  287   58  155    2   73  199    4   76    6   30   21    5\n",
      "  139    2   84   32  138   39    3  599    3   50    3   24    3    4\n",
      "   33    5   69    6   78   30   80   20   29   13   75   17    2  365\n",
      "   73    6  698    2  153   17    5   29   13  134    2   70   10  197\n",
      "    4  141    3  323  462    3   19   32   12    2  365   73  136  698\n",
      "   25  156   31    2   40  273    5  176  155    2   97  251   10  100\n",
      "   28   72    4  184  114    6  168    3   19  126   12    2   67  146\n",
      "   82   12    2   68  152    3  949   51  155    2    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "sample_data_tokenized = vectorize_layer(sample_data)\n",
    "print('Source text:')\n",
    "print(sample_data)\n",
    "print('\\n')\n",
    "print('Mapped sample:')\n",
    "print(sample_data_tokenized.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c4727-7bd6-49d3-a4b5-1ea3e37fd4bb",
   "metadata": {},
   "source": [
    "## 3. Create train/validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a717b2b3-68ce-4511-8c7a-6db64813f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map a single text slice into source and targets\n",
    "def map_src_tgt(text):\n",
    "    tokenized_sentence = vectorize_layer(text)\n",
    "    src = tokenized_sentence[:-1]\n",
    "    tgt = tokenized_sentence[1:]\n",
    "    return src, tgt\n",
    "    \n",
    "# create datasets\n",
    "def get_datasets(input_ds):\n",
    "    train_size = int(len(input_ds) * (1 - VALIDATION_SPLIT))\n",
    "    train_ds = input_ds.take(train_size) # take train dataset\n",
    "    valid_ds = input_ds.skip(train_size) # take validation dataset\n",
    "    print(f'train size: {train_ds.cardinality()}, valid size: {valid_ds.cardinality()}')\n",
    "\n",
    "    train_ds = train_ds.map(map_src_tgt)\n",
    "    valid_ds = valid_ds.map(map_src_tgt)\n",
    "    \n",
    "    train_ds = train_ds.batch(BATCH_SIZE).shuffle(1024).prefetch(1)\n",
    "    valid_ds = valid_ds.batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "    print(f'train batch: {train_ds.cardinality()}, valid batch: {valid_ds.cardinality()}')\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14bc2df-97b5-404c-b600-e6481f16a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 16078, valid size: 4020\n",
      "train batch: 503, valid batch: 126\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds = get_datasets(text_ds_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c4189-e96b-44d0-977e-c1eff2c61607",
   "metadata": {},
   "source": [
    "## 4. Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51f616bd-e50c-4c44-ab5b-fcfcc686bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                            LSTM_model Summary                                            \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                      \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│               │ LSTM_model │ \u001b[2mint32\u001b[0m[32,200]       │ \u001b[2mfloat32\u001b[0m[32,200,8200] │                              │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│ embed         │ Embed      │ \u001b[2mint32\u001b[0m[32,200]       │ \u001b[2mfloat32\u001b[0m[32,200,128]  │ embedding: \u001b[2mfloat32\u001b[0m[8200,128] │\n",
      "│               │            │                     │                      │                              │\n",
      "│               │            │                     │                      │ \u001b[1m1,049,600 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│ lstm_layers_0 │ RNN        │ \u001b[2mfloat32\u001b[0m[32,200,128] │ \u001b[2mfloat32\u001b[0m[32,200,128]  │ \u001b[1m131,584 \u001b[0m\u001b[1;2m(526.3 KB)\u001b[0m           │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│ dense         │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128] │ \u001b[2mfloat32\u001b[0m[32,200,8200] │ bias: \u001b[2mfloat32\u001b[0m[8200]          │\n",
      "│               │            │                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[128,8200]    │\n",
      "│               │            │                     │                      │                              │\n",
      "│               │            │                     │                      │ \u001b[1m1,057,800 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                   \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2,238,984 \u001b[0m\u001b[1;2m(9.0 MB)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────────┴────────────┴─────────────────────┴──────────────────────┴──────────────────────────────┘\n",
      "\u001b[1m                                                                                                          \u001b[0m\n",
      "\u001b[1m                                   Total Parameters: 2,238,984 \u001b[0m\u001b[1;2m(9.0 MB)\u001b[0m\u001b[1m                                   \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    num_lstm_layers: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(num_embeddings=VOCAB_SIZE, features=HIDDEN_DIM)\n",
    "\n",
    "        \n",
    "        self.lstm_layers = [nn.RNN(nn.OptimizedLSTMCell(), HIDDEN_DIM) \n",
    "                                for _ in range(self.num_lstm_layers)]\n",
    "        \n",
    "        self.dense = nn.Dense(features=VOCAB_SIZE)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Embedding\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0),\n",
    "        #                                                       batch_dims=(len(x),),\n",
    "        #                                                       size=HIDDEN_DIM)\n",
    "        # (carry, hidden), x = self.lstm((carry, hidden), x)\n",
    "        # variables = self.lstm.init(jax.random.PRNGKey(0), x)\n",
    "        for lstm in self.lstm_layers:\n",
    "            # x = self.lstm(x)\n",
    "            x = lstm(x)\n",
    "\n",
    "        # # LSTM\n",
    "        # for i in range(self.num_lstm_layers):\n",
    "        #     x = self.lstm_layers[i](x)\n",
    "        #     # hidden_state = self.lstm_layers[i].initialize_carry(jax.random.PRNGKey(0), (x.shape[0], x.shape[1]), HIDDEN_DIM)\n",
    "        #     # _, x = self.lstm_layers[i](hidden_state, x)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "lstm_model = LSTM_model(NUM_LSTM_LAYERS)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "print(lstm_model.tabulate(rng, jnp.ones((BATCH_SIZE, MAX_PAD_LEN), dtype=int), depth=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655caae-b24f-4b7d-aaf2-0f23e42fb42c",
   "metadata": {},
   "source": [
    "## 5. Create `TrainState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad1c0d0f-8e64-4757-9c40-9e00a000f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "# train state for lstm model\n",
    "def create_train_state(model, param_key, learning_rate):\n",
    "    # initialize model\n",
    "    params = model.init(param_key, jnp.ones((BATCH_SIZE, MAX_PAD_LEN), dtype=int))['params']\n",
    "    # initialize optimizer\n",
    "    tx = optax.adam(learning_rate=learning_rate)\n",
    "    return TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=params,\n",
    "            tx=tx,\n",
    "            metrics=Metrics.empty())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99e7e0-091a-43f8-88da-1c688debed12",
   "metadata": {},
   "source": [
    "## 6. Train step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1249a94d-8cde-4c3b-8117-5c4d2dee48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn({'params': params}, batch[0])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(preds, batch[1]).mean()\n",
    "        return loss\n",
    "\n",
    "    # compute loss and apply gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Update metrics\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state \n",
    "\n",
    "\n",
    "# evaluation\n",
    "@jax.jit\n",
    "def validation(state, batch):\n",
    "    preds = state.apply_fn({'params': state.params}, batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(preds, batch[1]).mean()\n",
    "\n",
    "    # Update metrics\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72be04d9-b2b7-4e96-a67e-e278976f93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next-word probability distribution\n",
    "@jax.jit\n",
    "def get_probs(state, input_tokens):\n",
    "    return state.apply_fn({'params': state.params}, input_tokens)[0][-1]\n",
    "\n",
    "\n",
    "# Text generator\n",
    "class TextGenerator():\n",
    "    def __init__(self, index_to_word):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {word : index for index, word in index_to_word.items()}\n",
    "\n",
    "    # scaling the model's output probability with temperature\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(VOCAB_SIZE, p=probs), probs\n",
    "    \n",
    "    # generate text\n",
    "    def generate(self, state, start_prompt, max_tokens, temperature, output_info=False):\n",
    "        \n",
    "        start_tokens = [self.word_to_index[word] for word in start_prompt.split()]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            input_tokens = np.array(start_tokens).reshape(1, -1)\n",
    "            probs = get_probs(state, input_tokens)\n",
    "            probs = nn.log_softmax(probs)\n",
    "            sample_token, probs = self.sample_from(np.exp(probs), temperature)\n",
    "            start_tokens.append(sample_token)\n",
    "            if output_info:\n",
    "                info.append({'tokens': np.copy(start_tokens), 'word_probs': probs})\n",
    "            \n",
    "        output_text = [self.index_to_word[token] for token in start_tokens if token != 0]\n",
    "        print(' '.join(output_text))\n",
    "\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea137ad-33ca-4525-bbd5-ed3097b49214",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ba8cf1f-9330-4a29-80b2-0c494b3327d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model(NUM_LSTM_LAYERS)\n",
    "state = create_train_state(lstm_model, jax.random.PRNGKey(0), learning_rate=LR)\n",
    "text_generator = TextGenerator(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99ebe744-3b35-4473-9e8a-35d81238c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-6916be85349e>:8: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
      "  state = train_step(state, batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tepoch time 0.21 min\n",
      "\ttrain loss: 4.0266, valid loss: 2.9790\n",
      "Epoch: 2\tepoch time 0.17 min\n",
      "\ttrain loss: 2.6842, valid loss: 2.4614\n",
      "Epoch: 3\tepoch time 0.18 min\n",
      "\ttrain loss: 2.3484, valid loss: 2.2507\n",
      "Epoch: 4\tepoch time 0.17 min\n",
      "\ttrain loss: 2.1758, valid loss: 2.1289\n",
      "Epoch: 5\tepoch time 0.17 min\n",
      "\ttrain loss: 2.0650, valid loss: 2.0471\n",
      "Epoch: 6\tepoch time 0.17 min\n",
      "\ttrain loss: 1.9834, valid loss: 1.9853\n",
      "Epoch: 7\tepoch time 0.18 min\n",
      "\ttrain loss: 1.9193, valid loss: 1.9347\n",
      "Epoch: 8\tepoch time 0.17 min\n",
      "\ttrain loss: 1.8673, valid loss: 1.8965\n",
      "Epoch: 9\tepoch time 0.18 min\n",
      "\ttrain loss: 1.8239, valid loss: 1.8637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mas_numpy_iterator():\n\u001b[0;32m----> 8\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mcompute()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mreplace(metrics\u001b[38;5;241m=\u001b[39mstate\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mempty())\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_JAX/lib/python3.9/site-packages/flax/core/frozen_dict.py:162\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, keys, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m   sorted_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict)\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    159\u001b[0m       [(jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mDictKey(k), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sorted_keys]\n\u001b[1;32m    160\u001b[0m   ), \u001b[38;5;28mtuple\u001b[39m(sorted_keys)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_unflatten\u001b[39m(\u001b[38;5;28mcls\u001b[39m, keys, values):\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;66;03m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    165\u001b[0m   \u001b[38;5;66;03m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, values)}, __unsafe_skip_copy__\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = defaultdict(list)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #training\n",
    "    for batch in train_ds.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    train_loss = state.metrics.compute()['loss']\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    #validation\n",
    "    test_state = state\n",
    "    for batch in valid_ds.as_numpy_iterator():\n",
    "        test_state = validation(test_state, batch)\n",
    "\n",
    "    valid_loss = test_state.metrics.compute()['loss']\n",
    "    \n",
    "    loss_hist['train_loss'].append(train_loss)\n",
    "    loss_hist['valid_loss'].append(valid_loss)\n",
    "\n",
    "    curr_time = time.time()\n",
    "    print(f'Epoch: {i+1}\\tepoch time {(curr_time - prev_time) / 60:.2f} min')\n",
    "    print(f'\\ttrain loss: {train_loss:.4f}, valid loss: {valid_loss:.4f}')\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        # generate text\n",
    "        print('\\nGenerated text:')\n",
    "        info = text_generator.generate(state, 'recipe for', MAX_VAL_TOKENS, 1.0)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e8861-4971-423a-81ab-5f38512465ee",
   "metadata": {},
   "source": [
    "## 8. Generate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2d4c7-c0d1-4972-9260-ac68b79e9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print prompt and top k candidate words probability\n",
    "def print_probs(info, index_to_word, top_k=5):\n",
    "    assert len(info) > 0, 'Please make `output_info=True`'\n",
    "    for i in range(len(info)):\n",
    "        start_tokens, word_probs = info[i].values()\n",
    "        start_prompts = [index_to_word[token] for token in start_tokens if token != 0]\n",
    "        start_prompts = ' '.join(start_prompts)\n",
    "        print(f'\\nPrompt: {start_prompts}')\n",
    "        # word_probs\n",
    "        probs_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
    "        for idx in probs_sorted:\n",
    "            print(f'{index_to_word[idx]}\\t{word_probs[idx] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73be6f-b081-45c1-b998-73d9ceeec322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate words probability with temperature = 1.0\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=6, \n",
    "                               temperature=1.0, \n",
    "                               output_info=True)\n",
    "\n",
    "print_probs(info, index_to_word, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2975f0c4-3c0e-4081-a365-83d9e83ec503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast until just tender\n",
      "\n",
      "Prompt: recipe for roast until\n",
      "until\t96.50%\n",
      ",\t1.59%\n",
      "in\t0.90%\n",
      "turkey\t0.54%\n",
      "chicken\t0.43%\n",
      "\n",
      "Prompt: recipe for roast until just\n",
      "just\t35.86%\n",
      "smooth\t24.25%\n",
      "golden\t17.78%\n",
      "the\t15.51%\n",
      "mixture\t2.17%\n",
      "\n",
      "Prompt: recipe for roast until just tender\n",
      "tender\t68.73%\n",
      "until\t16.24%\n",
      "cooked\t14.73%\n",
      "to\t0.12%\n",
      "before\t0.12%\n"
     ]
    }
   ],
   "source": [
    "# Candidate words probability distribution with temperature = 1.0\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=6, \n",
    "                               temperature=0.2, \n",
    "                               output_info=True)\n",
    "\n",
    "print_probs(info, index_to_word, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94def9a4-fc37-4154-877b-157f93039d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast them into 1 / 4 - poached chicken with garlic , tilting and put in a serrated knife . cover and ginger in 3 minutes . pour into 2 tablespoons fat remaining sauce warm water to 2 / 4 cup parsley . sprinkle with rice , and bring cherries soften to small bowl . do not crisp - mushroom mixture is smooth paste forms . broil for grilled eggplant is heated through , eggs until nuts are browned , and pepper . remove from heat and chopped fresh or until beginning to tear the beans in same\n"
     ]
    }
   ],
   "source": [
    "# generate text with temperature = 1.0\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=100, \n",
    "                               temperature=1.0, \n",
    "                               output_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4b2f17c-37e3-4990-a3b7-fa48cbe25f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast until golden , and cook , about 5 minutes . add the mixture . add the mixture . add the garlic , about 5 minutes . add the mixture and cook , and pepper . transfer to a large bowl . add the mixture over medium heat , about 10 minutes . add the mixture to a large bowl . add the mixture to a large bowl . add the mixture , and refrigerate . add the mixture to a large bowl . add the mixture to a large bowl . add the bowl . add the\n"
     ]
    }
   ],
   "source": [
    "# generate text with temperature = 0.2\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=100, \n",
    "                               temperature=0.2, \n",
    "                               output_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
