{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e1ee6e-4367-4ef1-b15c-9ad225053267",
   "metadata": {},
   "source": [
    "# LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e0b4f-1228-4eda-8842-9bf19873b034",
   "metadata": {},
   "source": [
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/01_lstm/lstm.ipynb)\n",
    "- Dataset: [Kaggle](https://www.kaggle.com/datasets/hugodarwood/epirecipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d625ea0-486e-4619-ad9d-0ebbd7c24d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "from clu import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76dd9c-8a94-4b81-907d-b0a64ee0c905",
   "metadata": {},
   "source": [
    "## 0. Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4684fb-c1d3-41b8-b7da-4ea8da5dfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/epirecipes/full_format_recipes.json'\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LSTM_LAYERS = 2\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "VOCAB_SIZE = 10000\n",
    "LR = 1e-3\n",
    "\n",
    "MAX_PAD_LEN = 200\n",
    "MAX_VAL_TOKENS = 100 # Max number of tokens when generating texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7785935-9cdb-4b7f-87d4-c62df452fa08",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c07bf4e-9e45-4811-a65c-b0e5378aeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(sentence):\n",
    "    sentence = re.sub(f'([{string.punctuation}])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad38cac-1ae6-42f7-95ef-3a9bff45e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open(DATA_DIR, 'r+') as f:\n",
    "    recipe_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f19bd7-9d03-4f72-8191-7d082c22663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recipe loaded: 20098\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "filtered_data = [\n",
    "    'Recipe for ' + x['title'] + ' | ' + ' '.join(x['directions'])\n",
    "    for x in recipe_data\n",
    "    if 'title' in x and x['title']\n",
    "    and 'directions' in x and x['directions']\n",
    "]\n",
    "\n",
    "text_ds = [pad_punctuation(sentence) for sentence in filtered_data]\n",
    "print(f'Total recipe loaded: {len(text_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bc4013-f1cb-46ea-81d1-abc8017365dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "Recipe for Shrimp Toasts With Sesame Seeds and Scallions | Pulse shrimp , chili paste , lemongrass , fish sauce , and ginger in a food processor until smooth . Season with salt and pulse again to combine . Transfer mixture to a medium bowl ; stir in scallion whites . Place sesame seeds on a plate . Spread shrimp mixture over bread slices , extending all the way to edges . Press bread , shrimp side down , into sesame seeds to coat evenly . Pour oil into a large skillet to come 1 / 4 \" up sides and heat over medium - high until a small pinch of shrimp mixture sizzles when added to oil . Working in 2 batches , fry toasts , shrimp side down , until golden and crisp , about 2 minutes ; turn and cook until other sides are golden and crisp , about 1 minute . Transfer to a paper towel–lined wire rack to drain . Cut each toast diagonally into quarters and top with scallion greens . \n"
     ]
    }
   ],
   "source": [
    "print('Sample data:')\n",
    "sample_data = np.random.choice(text_ds)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2809b15-a4bc-488c-a8cd-ae70f5fd1dae",
   "metadata": {},
   "source": [
    "## 2. Build vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef8cf15a-6a3c-4b16-a7f5-06426b294859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver texts list to tf dataset\n",
    "text_ds_tf = Dataset.from_tensor_slices(text_ds)\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_PAD_LEN+1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ccea64-db03-4e3b-b30c-6c2e4c88f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: .\n",
      "3: ,\n",
      "4: and\n",
      "5: to\n",
      "6: in\n",
      "7: the\n",
      "8: with\n",
      "9: a\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds_tf)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "index_to_word = {index : word for index, word in enumerate(vocab)}\n",
    "word_to_index = {word : index for index, word in enumerate(vocab)}\n",
    "\n",
    "# First 10 items in the vocabulary\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f'{i}: {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c385672-4086-4ddb-8113-e9efd56c2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text:\n",
      "Recipe for Shrimp Toasts With Sesame Seeds and Scallions | Pulse shrimp , chili paste , lemongrass , fish sauce , and ginger in a food processor until smooth . Season with salt and pulse again to combine . Transfer mixture to a medium bowl ; stir in scallion whites . Place sesame seeds on a plate . Spread shrimp mixture over bread slices , extending all the way to edges . Press bread , shrimp side down , into sesame seeds to coat evenly . Pour oil into a large skillet to come 1 / 4 \" up sides and heat over medium - high until a small pinch of shrimp mixture sizzles when added to oil . Working in 2 batches , fry toasts , shrimp side down , until golden and crisp , about 2 minutes ; turn and cook until other sides are golden and crisp , about 1 minute . Transfer to a paper towel–lined wire rack to drain . Cut each toast diagonally into quarters and top with scallion greens . \n",
      "\n",
      "\n",
      "Mapped sample:\n",
      "[  26   16  261  939    8  517  234    4  546   27  437  261    3  543\n",
      "  345    3 1176    3  213   54    3    4  272    6    9  291  188   10\n",
      "  141    2   63    8   24    4  437  626    5  103    2   40   31    5\n",
      "    9   29   21   22   42    6  870  383    2   64  517  234   28    9\n",
      "  219    2  166  261   31   20  195  160    3 3094  122    7 1194    5\n",
      "  333    2  322  195    3  261   96  198    3   25  517  234    5  163\n",
      "  239    2  107   37   25    9   30   56    5  953   11   23   32  338\n",
      "   99  129    4   17   20   29   13   75   10    9   65  602   14  261\n",
      "   31 2744  258 1642    5   37    2  407    6   15  303    3  366  939\n",
      "    3  261   96  198    3   10  100    4  253    3   19   15   12   22\n",
      "  202    4   43   10  555  129   79  100    4  253    3   19   11  164\n",
      "    2   40    5    9  148 2351  853  119    5  120    2   74   66  461\n",
      "  932   25 1056    4   72    8  870  472    2    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "sample_data_tokenized = vectorize_layer(sample_data)\n",
    "print('Source text:')\n",
    "print(sample_data)\n",
    "print('\\n')\n",
    "print('Mapped sample:')\n",
    "print(sample_data_tokenized.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c4727-7bd6-49d3-a4b5-1ea3e37fd4bb",
   "metadata": {},
   "source": [
    "## 3. Create train/validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a717b2b3-68ce-4511-8c7a-6db64813f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_src_tgt(text):\n",
    "    tokenized_sentence = vectorize_layer(text)\n",
    "    src = tokenized_sentence[:-1]\n",
    "    tgt = tokenized_sentence[1:]\n",
    "    return src, tgt\n",
    "    \n",
    "\n",
    "def get_datasets(input_ds):\n",
    "    train_size = int(len(input_ds) * (1 - VALIDATION_SPLIT))\n",
    "    train_ds = input_ds.take(train_size)\n",
    "    valid_ds = input_ds.skip(train_size)\n",
    "    print(f'train size: {train_ds.cardinality()}, valid size: {valid_ds.cardinality()}')\n",
    "\n",
    "    train_ds = train_ds.map(map_src_tgt)\n",
    "    valid_ds = valid_ds.map(map_src_tgt)\n",
    "    \n",
    "    train_ds = train_ds.batch(BATCH_SIZE).shuffle(1024).prefetch(1)\n",
    "    valid_ds = valid_ds.batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "    print(f'train batch: {train_ds.cardinality()}, valid batch: {valid_ds.cardinality()}')\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c14bc2df-97b5-404c-b600-e6481f16a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 16078, valid size: 4020\n",
      "train batch: 503, valid batch: 126\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds = get_datasets(text_ds_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c4189-e96b-44d0-977e-c1eff2c61607",
   "metadata": {},
   "source": [
    "## 4. Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51f616bd-e50c-4c44-ab5b-fcfcc686bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                              LSTM_model Summary                                              \u001b[0m\n",
      "┏━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│           │ LSTM_model │ \u001b[2mint32\u001b[0m[32,200]           │ \u001b[2mfloat32\u001b[0m[32,200,10000]   │                               │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ embed     │ Embed      │ \u001b[2mint32\u001b[0m[32,200]           │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ embedding: \u001b[2mfloat32\u001b[0m[10000,128] │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m1,280,000 \u001b[0m\u001b[1;2m(5.1 MB)\u001b[0m            │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0    │ LSTMCell   │ - - \u001b[2mfloat32\u001b[0m[32,200,128] │ - - \u001b[2mfloat32\u001b[0m[32,200,128] │                               │\n",
      "│           │            │   - \u001b[2mfloat32\u001b[0m[32,200,128] │   - \u001b[2mfloat32\u001b[0m[32,200,128] │                               │\n",
      "│           │            │ - \u001b[2mfloat32\u001b[0m[32,200,128]   │ - \u001b[2mfloat32\u001b[0m[32,200,128]   │                               │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/ii │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/hi │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/if │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/hf │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/ig │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/hg │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/io │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_0/ho │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1    │ LSTMCell   │ - - \u001b[2mfloat32\u001b[0m[32,200,128] │ - - \u001b[2mfloat32\u001b[0m[32,200,128] │                               │\n",
      "│           │            │   - \u001b[2mfloat32\u001b[0m[32,200,128] │   - \u001b[2mfloat32\u001b[0m[32,200,128] │                               │\n",
      "│           │            │ - \u001b[2mfloat32\u001b[0m[32,200,128]   │ - \u001b[2mfloat32\u001b[0m[32,200,128]   │                               │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/ii │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/hi │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/if │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/hf │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/ig │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/hg │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/io │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,384 \u001b[0m\u001b[1;2m(65.5 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ lstm_1/ho │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ bias: \u001b[2mfloat32\u001b[0m[128]            │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,128]      │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m16,512 \u001b[0m\u001b[1;2m(66.0 KB)\u001b[0m              │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│ dense     │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128]     │ \u001b[2mfloat32\u001b[0m[32,200,10000]   │ bias: \u001b[2mfloat32\u001b[0m[10000]          │\n",
      "│           │            │                         │                         │ kernel: \u001b[2mfloat32\u001b[0m[128,10000]    │\n",
      "│           │            │                         │                         │                               │\n",
      "│           │            │                         │                         │ \u001b[1m1,290,000 \u001b[0m\u001b[1;2m(5.2 MB)\u001b[0m            │\n",
      "├───────────┼────────────┼─────────────────────────┼─────────────────────────┼───────────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2,833,168 \u001b[0m\u001b[1;2m(11.3 MB)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────┴────────────┴─────────────────────────┴─────────────────────────┴───────────────────────────────┘\n",
      "\u001b[1m                                                                                                              \u001b[0m\n",
      "\u001b[1m                                    Total Parameters: 2,833,168 \u001b[0m\u001b[1;2m(11.3 MB)\u001b[0m\u001b[1m                                     \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    num_lstm_layers: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(num_embeddings=VOCAB_SIZE, features=HIDDEN_DIM)\n",
    "        self.lstm_layers = [nn.LSTMCell(name=f'lstm_{i}') for i in range(self.num_lstm_layers)]\n",
    "        self.dense = nn.Dense(features=VOCAB_SIZE)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Embedding\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # LSTM\n",
    "        for i in range(self.num_lstm_layers):\n",
    "            hidden_state = self.lstm_layers[i].initialize_carry(jax.random.PRNGKey(0), (x.shape[0], x.shape[1]), HIDDEN_DIM)\n",
    "            _, x = self.lstm_layers[i](hidden_state, x)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "lstm_model = LSTM_model(NUM_LSTM_LAYERS)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "print(lstm_model.tabulate(rng, jnp.ones((BATCH_SIZE, MAX_PAD_LEN), dtype=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655caae-b24f-4b7d-aaf2-0f23e42fb42c",
   "metadata": {},
   "source": [
    "## 5. Create `TrainState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1c0d0f-8e64-4757-9c40-9e00a000f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "\n",
    "def create_train_state(model, param_key, learning_rate):\n",
    "    # initialize model\n",
    "    params = model.init(param_key, jnp.ones((BATCH_SIZE, MAX_PAD_LEN), dtype=int))['params']\n",
    "    # initialize optimizer\n",
    "    tx = optax.adam(learning_rate=learning_rate)\n",
    "    return TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=params,\n",
    "            tx=tx,\n",
    "            metrics=Metrics.empty())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99e7e0-091a-43f8-88da-1c688debed12",
   "metadata": {},
   "source": [
    "## 6. Train step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1249a94d-8cde-4c3b-8117-5c4d2dee48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn({'params': params}, batch[0])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(preds, batch[1]).mean()\n",
    "        return loss\n",
    "\n",
    "    # compute loss and apply gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Update metrics\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state \n",
    "\n",
    "# evaluation\n",
    "@jax.jit\n",
    "def validation(state, batch):\n",
    "    preds = state.apply_fn({'params': state.params}, batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(preds, batch[1]).mean()\n",
    "\n",
    "    # Update metrics\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72be04d9-b2b7-4e96-a67e-e278976f93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next-word probability distribution\n",
    "@jax.jit\n",
    "def get_probs(state, input_tokens):\n",
    "    return state.apply_fn({'params': state.params}, input_tokens)[0][-1]\n",
    "\n",
    "# Text generator\n",
    "class TextGenerator():\n",
    "    def __init__(self, index_to_word):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {word : index for index, word in index_to_word.items()}\n",
    "\n",
    "    # scaling the model's output probability with temperature\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(VOCAB_SIZE, p=probs), probs\n",
    "    \n",
    "    # generate text\n",
    "    def generate(self, state, start_prompt, max_tokens, temperature, output_info=False):\n",
    "        \n",
    "        start_tokens = [self.word_to_index[word] for word in start_prompt.split()]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            input_tokens = np.array(start_tokens).reshape(1, -1)\n",
    "            probs = get_probs(state, input_tokens)\n",
    "            probs = nn.log_softmax(probs)\n",
    "            sample_token, probs = self.sample_from(np.exp(probs), temperature)\n",
    "            start_tokens.append(sample_token)\n",
    "            if output_info:\n",
    "                info.append({'tokens': np.copy(start_tokens), 'word_probs': probs})\n",
    "            \n",
    "        output_text = [self.index_to_word[token] for token in start_tokens if token != 0]\n",
    "        print(' '.join(output_text))\n",
    "\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea137ad-33ca-4525-bbd5-ed3097b49214",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ba8cf1f-9330-4a29-80b2-0c494b3327d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model(NUM_LSTM_LAYERS)\n",
    "state = create_train_state(lstm_model, jax.random.PRNGKey(0), learning_rate=LR)\n",
    "text_generator = TextGenerator(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99ebe744-3b35-4473-9e8a-35d81238c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tepoch time 0.09 min\n",
      "\ttrain loss: 4.4139, valid loss: 3.2973\n",
      "Epoch: 2\tepoch time 0.06 min\n",
      "\ttrain loss: 3.0422, valid loss: 2.8731\n",
      "Epoch: 3\tepoch time 0.06 min\n",
      "\ttrain loss: 2.8226, valid loss: 2.7693\n",
      "Epoch: 4\tepoch time 0.06 min\n",
      "\ttrain loss: 2.7436, valid loss: 2.7220\n",
      "Epoch: 5\tepoch time 0.06 min\n",
      "\ttrain loss: 2.7007, valid loss: 2.6938\n",
      "Epoch: 6\tepoch time 0.05 min\n",
      "\ttrain loss: 2.6722, valid loss: 2.6759\n",
      "Epoch: 7\tepoch time 0.06 min\n",
      "\ttrain loss: 2.6520, valid loss: 2.6625\n",
      "Epoch: 8\tepoch time 0.06 min\n",
      "\ttrain loss: 2.6369, valid loss: 2.6531\n",
      "Epoch: 9\tepoch time 0.06 min\n",
      "\ttrain loss: 2.6248, valid loss: 2.6453\n",
      "Epoch: 10\tepoch time 0.06 min\n",
      "\ttrain loss: 2.6149, valid loss: 2.6398\n",
      "\n",
      "Generated text:\n",
      "recipe for duck breasts , paprika , until coated and add chicken is heating pie dish ) chill for fish sauce , and mix 1 minute . place shiso . in single layer . finely chop chocolate - high speed until the grill for pasta in oats , uncovered until it , cinnamon , whisking until tender , knocking out , 1 cup cold water until tester inserted cream | heat butter into medium bowl . add guava juice to the baking sheets . transfer 1 tablespoon hot but 2 teaspoon freshly ground black bean mixture will become thick )\n",
      "\n",
      "\n",
      "Epoch: 11\tepoch time 0.06 min\n",
      "\ttrain loss: 2.6069, valid loss: 2.6362\n",
      "Epoch: 12\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5996, valid loss: 2.6324\n",
      "Epoch: 13\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5937, valid loss: 2.6290\n",
      "Epoch: 14\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5885, valid loss: 2.6253\n",
      "Epoch: 15\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5840, valid loss: 2.6247\n",
      "Epoch: 16\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5797, valid loss: 2.6220\n",
      "Epoch: 17\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5759, valid loss: 2.6207\n",
      "Epoch: 18\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5727, valid loss: 2.6197\n",
      "Epoch: 19\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5694, valid loss: 2.6186\n",
      "Epoch: 20\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5669, valid loss: 2.6165\n",
      "\n",
      "Generated text:\n",
      "recipe for parmesan soup roll , sugar in tiny eggs , preheat oven . do ahead . discard any clams . in sour down , lemon juice , in a heaping tablespoon olive oil , and quinoa into 1 bread from marinade , and herbs . top , 1 / 2 cup soufflé dishes ; preheat oven to 6 tablespoons lemon , beat eggs and candied garlic and toss onion and coarsely chop . ( 1 tablespoon soy sauce , salt and pepper . season sauce . mix almonds . beat in center , 1 stick gently stir in flour\n",
      "\n",
      "\n",
      "Epoch: 21\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5640, valid loss: 2.6154\n",
      "Epoch: 22\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5617, valid loss: 2.6150\n",
      "Epoch: 23\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5595, valid loss: 2.6142\n",
      "Epoch: 24\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5573, valid loss: 2.6130\n",
      "Epoch: 25\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5554, valid loss: 2.6138\n",
      "Epoch: 26\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5537, valid loss: 2.6125\n",
      "Epoch: 27\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5521, valid loss: 2.6119\n",
      "Epoch: 28\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5503, valid loss: 2.6112\n",
      "Epoch: 29\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5488, valid loss: 2.6118\n",
      "Epoch: 30\tepoch time 0.06 min\n",
      "\ttrain loss: 2.5476, valid loss: 2.6115\n",
      "\n",
      "Generated text:\n",
      "recipe for cinnamon , . bake until the olive oil in a small bowl to 1 inch round , at least 2 baking sheet of shell until lentils . plunge lobster stock and arugula salad . season with remaining praline can be made 5 minutes . cool syrup or 6 - cup chicken to warm or pot until cold and garlic . stack and sauté until cold other masa tightly covered . bring rice , and garlic and toss well charred and remaining lobster is just tender , dark brown , passing dough to pot or soufflé dishes ( shallots\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_hist = defaultdict(list)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #training\n",
    "    for batch in train_ds.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    train_loss = state.metrics.compute()['loss']\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    #validation\n",
    "    test_state = state\n",
    "    for batch in valid_ds.as_numpy_iterator():\n",
    "        test_state = validation(test_state, batch)\n",
    "\n",
    "    valid_loss = test_state.metrics.compute()['loss']\n",
    "    \n",
    "    \n",
    "    loss_hist['train_loss'].append(train_loss)\n",
    "    loss_hist['valid_loss'].append(valid_loss)\n",
    "\n",
    "    curr_time = time.time()\n",
    "    print(f'Epoch: {i+1}\\tepoch time {(curr_time - prev_time) / 60:.2f} min')\n",
    "    print(f'\\ttrain loss: {train_loss:.4f}, valid loss: {valid_loss:.4f}')\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        # generate text\n",
    "        print('\\nGenerated text:')\n",
    "        info = text_generator.generate(state, 'recipe for', MAX_VAL_TOKENS, 1.0)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910ba55-2566-46e3-8d87-e024471bb632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
