{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e1ee6e-4367-4ef1-b15c-9ad225053267",
   "metadata": {},
   "source": [
    "# LSTM on Recipe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e0b4f-1228-4eda-8842-9bf19873b034",
   "metadata": {},
   "source": [
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/01_lstm/lstm.ipynb)\n",
    "- Dataset: [Kaggle](https://www.kaggle.com/datasets/hugodarwood/epirecipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d625ea0-486e-4619-ad9d-0ebbd7c24d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "from clu import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76dd9c-8a94-4b81-907d-b0a64ee0c905",
   "metadata": {},
   "source": [
    "## 0. Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4684fb-c1d3-41b8-b7da-4ea8da5dfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/epirecipes/full_format_recipes.json'\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LSTM_LAYERS = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "VOCAB_SIZE = 8200\n",
    "LR = 1e-3\n",
    "\n",
    "MAX_PAD_LEN = 200\n",
    "MAX_VAL_TOKENS = 100 # Max number of tokens when generating texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7785935-9cdb-4b7f-87d4-c62df452fa08",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c07bf4e-9e45-4811-a65c-b0e5378aeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(sentence):\n",
    "    sentence = re.sub(f'([{string.punctuation}])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad38cac-1ae6-42f7-95ef-3a9bff45e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open(DATA_DIR, 'r+') as f:\n",
    "    recipe_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f19bd7-9d03-4f72-8191-7d082c22663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recipe loaded: 20098\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "filtered_data = [\n",
    "    'Recipe for ' + x['title'] + ' | ' + ' '.join(x['directions'])\n",
    "    for x in recipe_data\n",
    "    if 'title' in x and x['title']\n",
    "    and 'directions' in x and x['directions']\n",
    "]\n",
    "\n",
    "text_ds = [pad_punctuation(sentence) for sentence in filtered_data]\n",
    "print(f'Total recipe loaded: {len(text_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bc4013-f1cb-46ea-81d1-abc8017365dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "Recipe for Cucumber , Radish , and Red Onion Salad | Peel , halve , and seed cucumber . Diagonally cut cucumber into thin slices and cut radishes into julienne strips . In a bowl toss together all ingredients and season with salt and pepper . \n"
     ]
    }
   ],
   "source": [
    "print('Sample data:')\n",
    "sample_data = np.random.choice(text_ds)\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2809b15-a4bc-488c-a8cd-ae70f5fd1dae",
   "metadata": {},
   "source": [
    "## 2. Build vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8cf15a-6a3c-4b16-a7f5-06426b294859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver texts list to tf dataset\n",
    "text_ds_tf = Dataset.from_tensor_slices(text_ds)\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_PAD_LEN+1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ccea64-db03-4e3b-b30c-6c2e4c88f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: .\n",
      "3: ,\n",
      "4: and\n",
      "5: to\n",
      "6: in\n",
      "7: the\n",
      "8: with\n",
      "9: a\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds_tf)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "index_to_word = {index : word for index, word in enumerate(vocab)}\n",
    "word_to_index = {word : index for index, word in enumerate(vocab)}\n",
    "\n",
    "# First 10 items in the vocabulary\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f'{i}: {word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c385672-4086-4ddb-8113-e9efd56c2c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text:\n",
      "Recipe for Cucumber , Radish , and Red Onion Salad | Peel , halve , and seed cucumber . Diagonally cut cucumber into thin slices and cut radishes into julienne strips . In a bowl toss together all ingredients and season with salt and pepper . \n",
      "\n",
      "\n",
      "Mapped sample:\n",
      "[  26   16  569    3 1362    3    4  282  115  189   27  175    3  538\n",
      "    3    4  805  569    2  932   74  569   25  355  160    4   74  941\n",
      "   25 1710  393    2    6    9   21  117  110  122  131    4   63    8\n",
      "   24    4   33    2    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "sample_data_tokenized = vectorize_layer(sample_data)\n",
    "print('Source text:')\n",
    "print(sample_data)\n",
    "print('\\n')\n",
    "print('Mapped sample:')\n",
    "print(sample_data_tokenized.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4c4727-7bd6-49d3-a4b5-1ea3e37fd4bb",
   "metadata": {},
   "source": [
    "## 3. Create train/validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a717b2b3-68ce-4511-8c7a-6db64813f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map a single text slice into source and targets\n",
    "def map_src_tgt(text):\n",
    "    tokenized_sentence = vectorize_layer(text)\n",
    "    src = tokenized_sentence[:-1]\n",
    "    tgt = tokenized_sentence[1:]\n",
    "    return src, tgt\n",
    "    \n",
    "# create datasets\n",
    "def get_datasets(input_ds):\n",
    "    train_size = int(len(input_ds) * (1 - VALIDATION_SPLIT))\n",
    "    train_ds = input_ds.take(train_size) # take train dataset\n",
    "    valid_ds = input_ds.skip(train_size) # take validation dataset\n",
    "    print(f'train size: {train_ds.cardinality()}, valid size: {valid_ds.cardinality()}')\n",
    "\n",
    "    train_ds = train_ds.map(map_src_tgt)\n",
    "    valid_ds = valid_ds.map(map_src_tgt)\n",
    "    \n",
    "    train_ds = train_ds.batch(BATCH_SIZE).shuffle(1024).prefetch(1)\n",
    "    valid_ds = valid_ds.batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "    print(f'train batch: {train_ds.cardinality()}, valid batch: {valid_ds.cardinality()}')\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14bc2df-97b5-404c-b600-e6481f16a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 16078, valid size: 4020\n",
      "train batch: 503, valid batch: 126\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds = get_datasets(text_ds_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c4189-e96b-44d0-977e-c1eff2c61607",
   "metadata": {},
   "source": [
    "## 4. Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f616bd-e50c-4c44-ab5b-fcfcc686bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                            LSTM_model Summary                                            \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                      \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│               │ LSTM_model │ \u001b[2mint32\u001b[0m[32,200]       │ \u001b[2mfloat32\u001b[0m[32,200,8200] │                              │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│ embed         │ Embed      │ \u001b[2mint32\u001b[0m[32,200]       │ \u001b[2mfloat32\u001b[0m[32,200,128]  │ embedding: \u001b[2mfloat32\u001b[0m[8200,128] │\n",
      "│               │            │                     │                      │                              │\n",
      "│               │            │                     │                      │ \u001b[1m1,049,600 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│ lstm_layers_0 │ RNN        │ \u001b[2mfloat32\u001b[0m[32,200,128] │ \u001b[2mfloat32\u001b[0m[32,200,128]  │ \u001b[1m131,584 \u001b[0m\u001b[1;2m(526.3 KB)\u001b[0m           │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│ dense         │ Dense      │ \u001b[2mfloat32\u001b[0m[32,200,128] │ \u001b[2mfloat32\u001b[0m[32,200,8200] │ bias: \u001b[2mfloat32\u001b[0m[8200]          │\n",
      "│               │            │                     │                      │ kernel: \u001b[2mfloat32\u001b[0m[128,8200]    │\n",
      "│               │            │                     │                      │                              │\n",
      "│               │            │                     │                      │ \u001b[1m1,057,800 \u001b[0m\u001b[1;2m(4.2 MB)\u001b[0m           │\n",
      "├───────────────┼────────────┼─────────────────────┼──────────────────────┼──────────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                   \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2,238,984 \u001b[0m\u001b[1;2m(9.0 MB)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────────┴────────────┴─────────────────────┴──────────────────────┴──────────────────────────────┘\n",
      "\u001b[1m                                                                                                          \u001b[0m\n",
      "\u001b[1m                                   Total Parameters: 2,238,984 \u001b[0m\u001b[1;2m(9.0 MB)\u001b[0m\u001b[1m                                   \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LSTM_model(nn.Module):\n",
    "\n",
    "    num_lstm_layers: int\n",
    "    \n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(num_embeddings=VOCAB_SIZE, features=HIDDEN_DIM)\n",
    "        \n",
    "        self.lstm_layers = [nn.RNN(nn.OptimizedLSTMCell(), HIDDEN_DIM) \n",
    "                                for _ in range(self.num_lstm_layers)]\n",
    "        \n",
    "        self.dense = nn.Dense(features=VOCAB_SIZE)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Embedding\n",
    "        x = self.embed(x)\n",
    "        for lstm in self.lstm_layers:\n",
    "            x = lstm(x)\n",
    "        \n",
    "        # Dense layer\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "lstm_model = LSTM_model(NUM_LSTM_LAYERS)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "print(lstm_model.tabulate(rng, jnp.ones((BATCH_SIZE, MAX_PAD_LEN), dtype=int), depth=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655caae-b24f-4b7d-aaf2-0f23e42fb42c",
   "metadata": {},
   "source": [
    "## 5. Create `TrainState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad1c0d0f-8e64-4757-9c40-9e00a000f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "# train state for lstm model\n",
    "def create_train_state(model, param_key, learning_rate):\n",
    "    # initialize model\n",
    "    params = model.init(param_key, jnp.ones((BATCH_SIZE, MAX_PAD_LEN), dtype=int))['params']\n",
    "    # initialize optimizer\n",
    "    tx = optax.adam(learning_rate=learning_rate)\n",
    "    return TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=params,\n",
    "            tx=tx,\n",
    "            metrics=Metrics.empty())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99e7e0-091a-43f8-88da-1c688debed12",
   "metadata": {},
   "source": [
    "## 6. Train step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1249a94d-8cde-4c3b-8117-5c4d2dee48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        preds = state.apply_fn({'params': params}, batch[0])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(preds, batch[1]).mean()\n",
    "        return loss\n",
    "\n",
    "    # compute loss and apply gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Update metrics\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state \n",
    "\n",
    "\n",
    "# evaluation\n",
    "@jax.jit\n",
    "def validation(state, batch):\n",
    "    preds = state.apply_fn({'params': state.params}, batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(preds, batch[1]).mean()\n",
    "\n",
    "    # Update metrics\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72be04d9-b2b7-4e96-a67e-e278976f93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next-word probability distribution\n",
    "@jax.jit\n",
    "def get_probs(state, input_tokens):\n",
    "    return state.apply_fn({'params': state.params}, input_tokens)[0][-1]\n",
    "\n",
    "\n",
    "# Text generator\n",
    "class TextGenerator():\n",
    "    def __init__(self, index_to_word):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {word : index for index, word in index_to_word.items()}\n",
    "\n",
    "    # scaling the model's output probability with temperature\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(VOCAB_SIZE, p=probs), probs\n",
    "    \n",
    "    # generate text\n",
    "    def generate(self, state, start_prompt, max_tokens, temperature, output_info=False):\n",
    "        \n",
    "        start_tokens = [self.word_to_index[word] for word in start_prompt.split()]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            input_tokens = np.array(start_tokens).reshape(1, -1)\n",
    "            probs = get_probs(state, input_tokens)\n",
    "            probs = nn.log_softmax(probs)\n",
    "            sample_token, probs = self.sample_from(np.exp(probs), temperature)\n",
    "            start_tokens.append(sample_token)\n",
    "            if output_info:\n",
    "                info.append({'tokens': np.copy(start_tokens), 'word_probs': probs})\n",
    "            \n",
    "        output_text = [self.index_to_word[token] for token in start_tokens if token != 0]\n",
    "        print(' '.join(output_text))\n",
    "\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea137ad-33ca-4525-bbd5-ed3097b49214",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba8cf1f-9330-4a29-80b2-0c494b3327d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM_model(NUM_LSTM_LAYERS)\n",
    "state = create_train_state(lstm_model, jax.random.PRNGKey(0), learning_rate=LR)\n",
    "text_generator = TextGenerator(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99ebe744-3b35-4473-9e8a-35d81238c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tepoch time 0.21 min\n",
      "\ttrain loss: 3.9904, valid loss: 2.9524\n",
      "Epoch: 2\tepoch time 0.18 min\n",
      "\ttrain loss: 2.6645, valid loss: 2.4425\n",
      "Epoch: 3\tepoch time 0.18 min\n",
      "\ttrain loss: 2.3290, valid loss: 2.2334\n",
      "Epoch: 4\tepoch time 0.18 min\n",
      "\ttrain loss: 2.1584, valid loss: 2.1128\n",
      "Epoch: 5\tepoch time 0.18 min\n",
      "\ttrain loss: 2.0486, valid loss: 2.0321\n",
      "Epoch: 6\tepoch time 0.18 min\n",
      "\ttrain loss: 1.9692, valid loss: 1.9724\n",
      "Epoch: 7\tepoch time 0.18 min\n",
      "\ttrain loss: 1.9072, valid loss: 1.9247\n",
      "Epoch: 8\tepoch time 0.18 min\n",
      "\ttrain loss: 1.8567, valid loss: 1.8866\n",
      "Epoch: 9\tepoch time 0.18 min\n",
      "\ttrain loss: 1.8137, valid loss: 1.8557\n",
      "Epoch: 10\tepoch time 0.18 min\n",
      "\ttrain loss: 1.7761, valid loss: 1.8270\n",
      "\n",
      "Generated text:\n",
      "recipe for miso - water sauce | 1 . scrape them into an spice grinder . combine peaches , chili , and lemon zest in 2 inches . line with a bowl with nonstick oil . cook cumin and garlic in oil in same skillet over moderate heat , stirring , stirring , until wilted , about 5 minutes . remove from heat . add butter and sauté until all sauce is just wilted , about 1 minute . add chili powder , the reserved reserved 2 tablespoons butter , shallot , red pepper flakes , and ground black pepper\n",
      "\n",
      "\n",
      "Epoch: 11\tepoch time 0.18 min\n",
      "\ttrain loss: 1.7424, valid loss: 1.8037\n",
      "Epoch: 12\tepoch time 0.18 min\n",
      "\ttrain loss: 1.7133, valid loss: 1.7849\n",
      "Epoch: 13\tepoch time 0.18 min\n",
      "\ttrain loss: 1.6881, valid loss: 1.7693\n",
      "Epoch: 14\tepoch time 0.18 min\n",
      "\ttrain loss: 1.6653, valid loss: 1.7563\n",
      "Epoch: 15\tepoch time 0.18 min\n",
      "\ttrain loss: 1.6450, valid loss: 1.7436\n",
      "Epoch: 16\tepoch time 0.18 min\n",
      "\ttrain loss: 1.6265, valid loss: 1.7327\n",
      "Epoch: 17\tepoch time 0.18 min\n",
      "\ttrain loss: 1.6096, valid loss: 1.7246\n",
      "Epoch: 18\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5942, valid loss: 1.7166\n",
      "Epoch: 19\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5798, valid loss: 1.7087\n",
      "Epoch: 20\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5667, valid loss: 1.7038\n",
      "\n",
      "Generated text:\n",
      "recipe for moroccan lamb | cook creole seasoning in a hot - salted water to a boil over high heat until evenly brown and almost and vegetables are boiling , about 1 minute . season with salt and pepper . transfer to a bowl . thinly slice avocado and zest , grapefruit juice , and lime juice and puree into a food processor . using a vegetable peeler , peel potatoes with a sharp paring knife , add orange juice and basil to skillet until smooth . in another bowl whisk together mayonnaise , salt and parsley until mixture just\n",
      "\n",
      "\n",
      "Epoch: 21\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5543, valid loss: 1.6994\n",
      "Epoch: 22\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5422, valid loss: 1.6951\n",
      "Epoch: 23\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5328, valid loss: 1.6912\n",
      "Epoch: 24\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5210, valid loss: 1.6868\n",
      "Epoch: 25\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5114, valid loss: 1.6840\n",
      "Epoch: 26\tepoch time 0.18 min\n",
      "\ttrain loss: 1.5021, valid loss: 1.6821\n",
      "Epoch: 27\tepoch time 0.18 min\n",
      "\ttrain loss: 1.4933, valid loss: 1.6805\n",
      "Epoch: 28\tepoch time 0.18 min\n",
      "\ttrain loss: 1.4854, valid loss: 1.6770\n",
      "Epoch: 29\tepoch time 0.18 min\n",
      "\ttrain loss: 1.4772, valid loss: 1.6756\n",
      "Epoch: 30\tepoch time 0.18 min\n",
      "\ttrain loss: 1.4697, valid loss: 1.6750\n",
      "\n",
      "Generated text:\n",
      "recipe for minted ditalini sauce | preheat oven to 425°f . gently mix 1 / 4 cup fresh oyster concentrate , remaining 3 / 4 cup butter , eggs , and shallot in a small bowl . toss well - seasoned flour in a 13 - by 9 - inch baking pan . preheat oven to 475° . remove the rolls from grill and reserve . cut into 1 / 16 - inch pieces and in a small bowl toss ingredients with remaining teaspoon salt and shake , reserving . add to blue tomato sauce and stir until emulsified .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_hist = defaultdict(list)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    #training\n",
    "    for batch in train_ds.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    train_loss = state.metrics.compute()['loss']\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    #validation\n",
    "    test_state = state\n",
    "    for batch in valid_ds.as_numpy_iterator():\n",
    "        test_state = validation(test_state, batch)\n",
    "\n",
    "    valid_loss = test_state.metrics.compute()['loss']\n",
    "    \n",
    "    loss_hist['train_loss'].append(train_loss)\n",
    "    loss_hist['valid_loss'].append(valid_loss)\n",
    "\n",
    "    curr_time = time.time()\n",
    "    print(f'Epoch: {i+1}\\tepoch time {(curr_time - prev_time) / 60:.2f} min')\n",
    "    print(f'\\ttrain loss: {train_loss:.4f}, valid loss: {valid_loss:.4f}')\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        # generate text\n",
    "        print('\\nGenerated text:')\n",
    "        info = text_generator.generate(state, 'recipe for', MAX_VAL_TOKENS, 1.0)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e8861-4971-423a-81ab-5f38512465ee",
   "metadata": {},
   "source": [
    "## 8. Generate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06a2d4c7-c0d1-4972-9260-ac68b79e9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print prompt and top k candidate words probability\n",
    "def print_probs(info, index_to_word, top_k=5):\n",
    "    assert len(info) > 0, 'Please make `output_info=True`'\n",
    "    for i in range(len(info)):\n",
    "        start_tokens, word_probs = info[i].values()\n",
    "        start_prompts = [index_to_word[token] for token in start_tokens if token != 0]\n",
    "        start_prompts = ' '.join(start_prompts)\n",
    "        print(f'\\nPrompt: {start_prompts}')\n",
    "        # word_probs\n",
    "        probs_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
    "        for idx in probs_sorted:\n",
    "            print(f'{index_to_word[idx]}\\t{word_probs[idx] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b73be6f-b081-45c1-b998-73d9ceeec322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast turkey with au\n",
      "\n",
      "Prompt: recipe for roast turkey\n",
      "chicken\t22.24%\n",
      "pork\t16.31%\n",
      "turkey\t14.29%\n",
      "beef\t9.31%\n",
      "lamb\t4.78%\n",
      "\n",
      "Prompt: recipe for roast turkey with\n",
      "with\t91.31%\n",
      "breast\t1.43%\n",
      "and\t0.74%\n",
      "|\t0.58%\n",
      "legs\t0.31%\n",
      "\n",
      "Prompt: recipe for roast turkey with au\n",
      "lemon\t2.73%\n",
      "port\t2.62%\n",
      "red\t1.99%\n",
      "roasted\t1.95%\n",
      "salt\t1.81%\n"
     ]
    }
   ],
   "source": [
    "# Candidate words probability with temperature = 1.0\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=6, \n",
    "                               temperature=1.0, \n",
    "                               output_info=True)\n",
    "\n",
    "print_probs(info, index_to_word, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2975f0c4-3c0e-4081-a365-83d9e83ec503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast chicken with roasted\n",
      "\n",
      "Prompt: recipe for roast chicken\n",
      "chicken\t74.93%\n",
      "pork\t15.88%\n",
      "turkey\t8.19%\n",
      "beef\t0.96%\n",
      "lamb\t0.03%\n",
      "\n",
      "Prompt: recipe for roast chicken with\n",
      "with\t100.00%\n",
      "|\t0.00%\n",
      "and\t0.00%\n",
      "in\t0.00%\n",
      "breasts\t0.00%\n",
      "\n",
      "Prompt: recipe for roast chicken with roasted\n",
      "lemon\t40.40%\n",
      "roasted\t31.21%\n",
      "red\t5.84%\n",
      "mustard\t4.11%\n",
      "white\t3.64%\n"
     ]
    }
   ],
   "source": [
    "# Candidate words probability distribution with temperature = 1.0\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=6, \n",
    "                               temperature=0.2, \n",
    "                               output_info=True)\n",
    "\n",
    "print_probs(info, index_to_word, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94def9a4-fc37-4154-877b-157f93039d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast pork chops with pimiento cheese and basil | cook guajillo and onion in large pot of boiling salted water 30 seconds . drain potatoes in well . transfer carrots to large baking sheet . lift off skin and discard . add sliced prunes to outside of 9 - inch - diameter glass dish with pan juices . stir in butter and chile paste . increase heat by 1 cup , thyme , onion , cardamom , and cloves , necks and peppercorns to hot water . cover , and cook until potatoes are tender , about 8\n"
     ]
    }
   ],
   "source": [
    "# generate text with temperature = 1.0\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=100, \n",
    "                               temperature=1.0, \n",
    "                               output_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4b2f17c-37e3-4990-a3b7-fa48cbe25f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe for roast chicken with ginger - ginger butter | preheat oven to 350°f . butter and flour a 9 - inch - diameter glass pie dish . combine first 6 ingredients in small bowl . add 1 / 2 cup oil ; rub with remaining 1 / 4 cup oil . place chicken on baking sheet . roast until tender , about 20 minutes . transfer to large rimmed baking sheet . add remaining 1 / 4 cup oil ; rub in pan . roast until thermometer inserted into thickest part of thigh registers 165°f , about 35 minutes\n"
     ]
    }
   ],
   "source": [
    "# generate text with temperature = 0.2\n",
    "info = text_generator.generate(state, \n",
    "                               'recipe for roast', \n",
    "                               max_tokens=100, \n",
    "                               temperature=0.2, \n",
    "                               output_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
