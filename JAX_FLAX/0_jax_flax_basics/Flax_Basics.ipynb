{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c62c25",
   "metadata": {},
   "source": [
    "This notebook follows the instruction on: https://flax.readthedocs.io/en/latest/guides/flax_basics.html#linear-regression-with-flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3f446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import jit\n",
    "from typing import Any, Callable, Sequence\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829956ed",
   "metadata": {},
   "source": [
    "# 1. Linear Regression with Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f21d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dense layer instance\n",
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f73c66",
   "metadata": {},
   "source": [
    "### Model parameters & initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940ac31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (5,),\n",
       "        kernel: (10, 5),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (10, )) # this x is used to trigger shape inference\n",
    "params = model.init(key2, x)\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83342cd0",
   "metadata": {},
   "source": [
    "Parameters are stored in a `FrozenDict` instance which deal with the function nature of JAX by preventing any mutation of the underlying dict and making the user aware of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca573d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    params: {\n",
      "        kernel: Array([[ 2.35571593e-01, -1.71652630e-01, -4.45728898e-02,\n",
      "                -4.68043625e-01,  4.54595298e-01],\n",
      "               [-6.87736511e-01,  3.67835432e-01, -1.79262117e-01,\n",
      "                 1.29276216e-01, -2.42580175e-01],\n",
      "               [ 2.02303097e-01, -2.49465629e-01,  2.74955630e-01,\n",
      "                 4.73488301e-01, -1.98002532e-01],\n",
      "               [ 2.74478376e-01, -1.21369645e-01, -2.25361690e-01,\n",
      "                -4.78193611e-01, -9.63979959e-02],\n",
      "               [-6.19886220e-02, -1.72743499e-01,  2.96947401e-04,\n",
      "                -7.17593431e-01,  2.00894251e-01],\n",
      "               [-5.60321212e-01,  3.27208459e-01,  1.06281511e-01,\n",
      "                 1.28758654e-01,  1.16973273e-01],\n",
      "               [ 1.82219014e-01,  1.11444041e-01, -1.62924170e-01,\n",
      "                 3.24953273e-02, -1.67053357e-01],\n",
      "               [ 4.31294084e-01,  2.08004534e-01,  1.47714198e-01,\n",
      "                -8.51502791e-02, -1.26487076e-01],\n",
      "               [ 3.29497337e-01,  1.08470373e-01, -4.01340097e-01,\n",
      "                 1.66956067e-01,  5.74723661e-01],\n",
      "               [-3.84744763e-01, -3.75315577e-01, -5.35782166e-02,\n",
      "                -2.51350939e-01, -4.78640795e-01]], dtype=float32),\n",
      "        bias: Array([0., 0., 0., 0., 0.], dtype=float32),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682d4b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense(\n",
      "    # attributes\n",
      "    features = 5\n",
      "    use_bias = True\n",
      "    dtype = None\n",
      "    param_dtype = float32\n",
      "    precision = None\n",
      "    kernel_init = init\n",
      "    bias_init = zeros\n",
      "    dot_general = dot_general\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b57fd",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0a5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (20, 10) ; y shape (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "\n",
    "# Store the parameters in a pytree\n",
    "true_params = freeze({'params':{'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
    "print('x shape: ', x_samples.shape, '; y shape', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfcf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "    # Define square loss for a single pair\n",
    "    def squared_error(x, y):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.inner(y-pred, y-pred) / 2.0\n",
    "    # Vectorize the previous to compute the average of the loss on all samples\n",
    "    return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34add1d7",
   "metadata": {},
   "source": [
    "Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4798e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W, b:  0.023639798\n",
      "Loss step 0:  35.343876\n",
      "Loss step 10:  0.5143469\n",
      "Loss step 20:  0.11384161\n",
      "Loss step 30:  0.03932675\n",
      "Loss step 40:  0.019916205\n",
      "Loss step 50:  0.014209128\n",
      "Loss step 60:  0.012425651\n",
      "Loss step 70:  0.0118503915\n",
      "Loss step 80:  0.011661774\n",
      "Loss step 90:  0.011599411\n",
      "Loss step 100:  0.011578695\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "print('Loss for \"true\" W, b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "    params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads\n",
    "    )\n",
    "    return params\n",
    "\n",
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    params = update_params(params, learning_rate, grads)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d260835",
   "metadata": {},
   "source": [
    "### Optimizing with Optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fb751",
   "metadata": {},
   "source": [
    "Basic usage of Optax:\n",
    "1. Choose an optimization method (e.g. `optax.adam`);\n",
    "2. Create optimizer state from parameters (for the Adam optimizer, this state will contain the *momentum* values);\n",
    "3. Compute the gradients of the loss with `jax.value_and_grad()`;\n",
    "4. At every iteration, call the Optax `update` function to update the internal optimizer state and create an update to the parameters. Then add the update to the parameters with Optax's `apply_updates` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da75b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.011577623\n",
      "Loss step 10:  0.2614426\n",
      "Loss step 20:  0.076756336\n",
      "Loss step 30:  0.03644162\n",
      "Loss step 40:  0.022014325\n",
      "Loss step 50:  0.016178917\n",
      "Loss step 60:  0.013002919\n",
      "Loss step 70:  0.012026143\n",
      "Loss step 80:  0.0117644435\n",
      "Loss step 90:  0.011646055\n",
      "Loss step 100:  0.011585526\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f35693",
   "metadata": {},
   "source": [
    "### Serializing the result\n",
    "Save and load model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b11d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict output\n",
      "{'params': {'bias': Array([-1.4555762, -2.0277987,  2.079098 ,  1.2186146, -0.9980971],      dtype=float32), 'kernel': Array([[ 1.0098814 ,  0.18934432,  0.04455043, -0.9280222 ,  0.34784007],\n",
      "       [ 1.7298449 ,  0.98793644,  1.164046  ,  1.1006075 , -0.10653944],\n",
      "       [-1.2029458 ,  0.2863525 ,  1.4155985 ,  0.11871001, -1.3141481 ],\n",
      "       [-1.1941485 , -0.18958484,  0.0341387 ,  1.3169428 ,  0.08060349],\n",
      "       [ 0.13852426,  1.3713043 , -1.3187183 ,  0.53152704, -2.2404993 ],\n",
      "       [ 0.56293976,  0.812231  ,  0.31751972,  0.53455067,  0.90500313],\n",
      "       [-0.37926075,  1.7410388 ,  1.0790282 , -0.50398386,  0.92830575],\n",
      "       [ 0.97064954, -1.3153397 ,  0.33681548,  0.80993474, -1.2018455 ],\n",
      "       [ 1.0194305 , -0.62024856,  1.081882  , -1.8389751 , -0.45805126],\n",
      "       [-0.6436538 ,  0.4566669 , -1.1329137 , -0.6853869 ,  0.16829033]],      dtype=float32)}}\n",
      "Bytes output\n",
      "b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14RP\\xba\\xbft\\xc7\\x01\\xc0\\xf1\\x0f\\x05@\\x90\\xfb\\x9b?K\\x83\\x7f\\xbf\\xa6kernel\\xc7\\xd6\\x01\\x93\\x92\\n\\x05\\xa7float32\\xc4\\xc8\\xcbC\\x81?z\\xe3A>\\x82z6=\\xdd\\x92m\\xbf\\x18\\x18\\xb2>\\x8fk\\xdd?g\\xe9|?v\\xff\\x94?\\xb5\\xe0\\x8c?Z1\\xda\\xbd!\\xfa\\x99\\xbf\\xcb\\x9c\\x92>U2\\xb5?<\\x1e\\xf3=\\x016\\xa8\\xbf\\xdc\\xd9\\x98\\xbf\\x87\"B\\xbe\\x05\\xd5\\x0b=\\x95\\x91\\xa8?q\\x13\\xa5=N\\xd9\\r>\\xe6\\x86\\xaf?\\xc3\\xcb\\xa8\\xbf(\\x12\\x08?Wd\\x0f\\xc0\\xd2\\x1c\\x10?_\\xeeO?\\xf2\\x91\\xa2>P\\xd8\\x08?I\\xaeg?w.\\xc2\\xbe\\\\\\xda\\xde?\\x99\\x1d\\x8a?\\x16\\x05\\x01\\xbfr\\xa5m?}|x?\\r]\\xa8\\xbf\\x14s\\xac>\\xe2WO?\\x13\\xd6\\x99\\xbf\\xb3|\\x82?\\x9c\\xc8\\x1e\\xbf\\x1c{\\x8a?\\x89c\\xeb\\xbf\\xb2\\x85\\xea\\xbe\\x7f\\xc6$\\xbf>\\xd0\\xe9>Q\\x03\\x91\\xbf\\x84u/\\xbfMT,>'\n"
     ]
    }
   ],
   "source": [
    "from flax import serialization\n",
    "bytes_output = serialization.to_bytes(params)\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "\n",
    "print('Dict output')\n",
    "print(dict_output)\n",
    "\n",
    "print('Bytes output')\n",
    "print(bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34d6aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: array([-1.4555762, -2.0277987,  2.079098 ,  1.2186146, -0.9980971],\n",
       "              dtype=float32),\n",
       "        kernel: array([[ 1.0098814 ,  0.18934432,  0.04455043, -0.9280222 ,  0.34784007],\n",
       "               [ 1.7298449 ,  0.98793644,  1.164046  ,  1.1006075 , -0.10653944],\n",
       "               [-1.2029458 ,  0.2863525 ,  1.4155985 ,  0.11871001, -1.3141481 ],\n",
       "               [-1.1941485 , -0.18958484,  0.0341387 ,  1.3169428 ,  0.08060349],\n",
       "               [ 0.13852426,  1.3713043 , -1.3187183 ,  0.53152704, -2.2404993 ],\n",
       "               [ 0.56293976,  0.812231  ,  0.31751972,  0.53455067,  0.90500313],\n",
       "               [-0.37926075,  1.7410388 ,  1.0790282 , -0.50398386,  0.92830575],\n",
       "               [ 0.97064954, -1.3153397 ,  0.33681548,  0.80993474, -1.2018455 ],\n",
       "               [ 1.0194305 , -0.62024856,  1.081882  , -1.8389751 , -0.45805126],\n",
       "               [-0.6436538 ,  0.4566669 , -1.1329137 , -0.6853869 ,  0.16829033]],\n",
       "              dtype=float32),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialization.from_bytes(params, bytes_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1dd12",
   "metadata": {},
   "source": [
    "# Defining your own models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8610fe19",
   "metadata": {},
   "source": [
    "### Module basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4908f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723787 -0.00810345 -0.0255093   0.02151708 -0.01261237]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class ExplicitMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "        \n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, lyr in enumerate(self.layers):\n",
    "            x = lyr(x)\n",
    "            if i != len(self.layers) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4, 4))\n",
    "\n",
    "model = ExplicitMLP(features=[3, 4, 5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialize parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1c3dd",
   "metadata": {},
   "source": [
    "Since **the modules structure and its parameters are not tied to each other**, we cannot directly call `model(x)` on a given input as it will return an error. The `__call__` function is being wrapped up in the `apply` one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6136b3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ExplicitMLP\" object has no attribute \"layers\". If \"layers\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y = model(x)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd0f6861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723787 -0.00810345 -0.0255093   0.02151708 -0.01261237]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for i, feat in enumerate(self.features):\n",
    "            x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "            if i != len(self.features) - 1:\n",
    "                x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4, 4))\n",
    "\n",
    "model = SimpleMLP(features=[3, 4, 5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter:\\n', jax.tree_util.tree_map(jnp.shape, unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f30dc7b7-fe78-4fa5-91e2-52157141f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameters:\n",
      " FrozenDict({\n",
      "    params: {\n",
      "        kernel: Array([[ 0.61506   , -0.22728713,  0.60547006],\n",
      "               [-0.2961799 ,  1.1232013 , -0.879759  ],\n",
      "               [-0.35162622,  0.38064912,  0.68932474],\n",
      "               [-0.1151355 ,  0.04567899, -1.091212  ]], dtype=float32),\n",
      "        bias: Array([0., 0., 0.], dtype=float32),\n",
      "    },\n",
      "})\n",
      "output:\n",
      " [[-0.029962    1.102088   -0.6660265 ]\n",
      " [-0.31092793  0.6323942  -0.5367881 ]\n",
      " [ 0.0142401   0.9424717  -0.6356147 ]\n",
      " [ 0.36818963  0.35865188 -0.00459227]]\n"
     ]
    }
   ],
   "source": [
    "class SimpleDense(nn.Module):\n",
    "    features: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        kernel = self.param('kernel',\n",
    "                            self.kernel_init,\n",
    "                            (inputs.shape[-1], self.features))\n",
    "\n",
    "        y = lax.dot_general(inputs, kernel,\n",
    "                            (((inputs.ndim-1,), (0,)),((), ())),)\n",
    "        \n",
    "        bias = self.param('bias',\n",
    "                          self.bias_init,\n",
    "                          (self.features))\n",
    "\n",
    "        y = y + bias\n",
    "        return y\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = random.uniform(key1, (4, 4))\n",
    "\n",
    "model = SimpleDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13261c2-1188-48bb-af7f-7787849e2529",
   "metadata": {},
   "source": [
    "### Variables and collections of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32972b7-5572-41e1-b79b-93ae97b70b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized variables:\n",
      " FrozenDict({\n",
      "    batch_stats: {\n",
      "        mean: Array([0., 0., 0., 0., 0.], dtype=float32),\n",
      "    },\n",
      "    params: {\n",
      "        bias: Array([0., 0., 0., 0., 0.], dtype=float32),\n",
      "    },\n",
      "})\n",
      "updated_state:\n",
      " FrozenDict({\n",
      "    batch_stats: {\n",
      "        mean: Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "class BiasAdderWithRunningMean(nn.Module):\n",
    "    decay: float = 0.99\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # easy pattern to detect if we're initializing via empty variable tree\n",
    "        is_initialized = self.has_variable('batch_stats', 'mean')\n",
    "        ra_mean = self.variable('batch_stats', 'mean',\n",
    "                                lambda s:jnp.zeros(s),\n",
    "                                x.shape[1:])\n",
    "        mean = ra_mean.value\n",
    "        bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
    "        \n",
    "        if is_initialized:\n",
    "            ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n",
    "\n",
    "        return x - ra_mean.value + bias\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
    "x = jnp.ones((10, 5))\n",
    "model = BiasAdderWithRunningMean()\n",
    "variables = model.init(key1, x)\n",
    "\n",
    "print('initialized variables:\\n', variables)\n",
    "y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
    "print('updated_state:\\n', updated_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
