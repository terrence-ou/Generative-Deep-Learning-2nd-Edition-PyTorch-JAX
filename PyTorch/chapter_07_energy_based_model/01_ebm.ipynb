{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d037be91-b62d-49d2-a6a0-e1e4470a61a7",
   "metadata": {},
   "source": [
    "# Energy-based Model (In-progress)\n",
    "\n",
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184?keywords=generative+deep+learning,+2nd+edition&qid=1684708209&sprefix=generative+de,aps,93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/07_ebm/01_ebm/ebm.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17455117-10a3-4c32-aae4-357865356b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as Transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "from torcheval import metrics as Metrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\" # this is only for Mac metal chips\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f6abf-15c4-454a-9945-e7c3707c4b7a",
   "metadata": {},
   "source": [
    "## 0. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d818e38-1314-4545-aa1e-fa8633e716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "STEP_SIZE = 10\n",
    "STEPS = 60\n",
    "NOISE = 5e-3\n",
    "ALPHA = 0.1\n",
    "GRADIENT_CLIP = 3e-2\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 8192\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d84158-5ed5-4bd1-bfd3-e847429a2ca7",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0781de4-e5b4-416e-9094-bc2f7b580f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "\n",
    "    transform = Transforms.Compose([\n",
    "                    Transforms.ToTensor(),\n",
    "                    Transforms.Resize(IMAGE_SIZE, antialias=True),\n",
    "                    Transforms.Normalize((0.5), (0.5)),])\n",
    "\n",
    "    train_ds = torchvision.datasets.MNIST(\"./data\", train=True,\n",
    "                                          download=True,\n",
    "                                          transform=transform)\n",
    "    test_ds = torchvision.datasets.MNIST(\"./data\", train=False,\n",
    "                                          download=True,\n",
    "                                          transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=4)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=4)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e78af0-1b61-4bc3-9302-7418998abf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# check dataset and dataloader\n",
    "temp_loader, _ = get_dataloader()\n",
    "print(next(iter(temp_loader))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bf25699-0eae-42a2-87de-973d96f29cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = next(iter(temp_loader))\n",
    "test_fig = test_imgs[0][0]\n",
    "test_fig = test_fig.data.clamp_(min=-1.0, max=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037158a-44c4-4d39-9226-ebd0dc77f4bf",
   "metadata": {},
   "source": [
    "## 2. Building Energy Function $E(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ff72095-8d40-4255-bf9f-00d90446666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Energy(nn.Module):\n",
    "\n",
    "    conv_layers = 4\n",
    "    channels:list = [1, 16, 32, 64, 64]\n",
    "    kernels:list = [5, 3, 3, 3]\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        # Adding convolutional layers + Swish activations\n",
    "        for i in range(self.conv_layers):\n",
    "            modules.append(nn.Conv2d(in_channels=self.channels[i], \n",
    "                                     out_channels=self.channels[i+1], \n",
    "                                     kernel_size=self.kernels[i],\n",
    "                                     stride=2,\n",
    "                                     padding=self.kernels[i] // 2))\n",
    "            modules.append(nn.SiLU())\n",
    "\n",
    "        # Adding linear layers\n",
    "        modules += [nn.Flatten(),\n",
    "                    nn.Linear(in_features=256, out_features=64),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(64, 1)]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9b03911-f328-4b13-871d-dc0b80ddcd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Energy                                   [32, 1]                   --\n",
       "├─Sequential: 1-1                        [32, 1]                   --\n",
       "│    └─Conv2d: 2-1                       [32, 16, 16, 16]          416\n",
       "│    └─SiLU: 2-2                         [32, 16, 16, 16]          --\n",
       "│    └─Conv2d: 2-3                       [32, 32, 8, 8]            4,640\n",
       "│    └─SiLU: 2-4                         [32, 32, 8, 8]            --\n",
       "│    └─Conv2d: 2-5                       [32, 64, 4, 4]            18,496\n",
       "│    └─SiLU: 2-6                         [32, 64, 4, 4]            --\n",
       "│    └─Conv2d: 2-7                       [32, 64, 2, 2]            36,928\n",
       "│    └─SiLU: 2-8                         [32, 64, 2, 2]            --\n",
       "│    └─Flatten: 2-9                      [32, 256]                 --\n",
       "│    └─Linear: 2-10                      [32, 64]                  16,448\n",
       "│    └─SiLU: 2-11                        [32, 64]                  --\n",
       "│    └─Linear: 2-12                      [32, 1]                   65\n",
       "==========================================================================================\n",
       "Total params: 76,993\n",
       "Trainable params: 76,993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 27.64\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 1.92\n",
       "Params size (MB): 0.31\n",
       "Estimated Total Size (MB): 2.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Energy(), input_size=(32, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96287321-2940-4f2c-805b-2c4d65adc28f",
   "metadata": {},
   "source": [
    "## 3. Setting Up Langevin Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc00ff1d-87fb-4515-b9b1-bdeadbf7553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still a lot of uncertainty in this block\n",
    "\n",
    "def generate_samples(\n",
    "        model, input_imgs, steps, step_size, noise, return_img_per_step=False):\n",
    "    \n",
    "    #turn off the gradient for the model; we only need gradients for the input image\n",
    "    model_state = model.training\n",
    "    grad_state = torch.is_grad_enabled()\n",
    "    torch.set_grad_enabled(True)\n",
    "    noise = torch.randn(input_imgs.shape, device=input_imgs.device)\n",
    "    \n",
    "    model.eval()\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = False\n",
    "    input_imgs.requires_grad = True\n",
    "        \n",
    "    imgs_per_step = []\n",
    "    for _ in range(steps):\n",
    "        # step 1: add noise to the input image\n",
    "        noise.normal_(0, 0.005)\n",
    "        input_imgs.data.add_(noise.data)\n",
    "        input_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "        # step 2: calculate gradients for the current input\n",
    "        output_imgs = model(input_imgs)\n",
    "        output_imgs.sum().backward() # is `sum` really necessary?\n",
    "        input_imgs.grad.data.clamp_(-GRADIENT_CLIP, GRADIENT_CLIP) # preventing gradient explosion or vanishing\n",
    "\n",
    "        # apply gradients to the current sample\n",
    "        input_imgs.data.add_(step_size * input_imgs.grad.data)\n",
    "        input_imgs.grad.detach_()\n",
    "        input_imgs.grad.zero_()\n",
    "        input_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(input_imgs.clone().detach())\n",
    "\n",
    "    # reset model states\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = True\n",
    "    \n",
    "    if model_state: model.train()\n",
    "    torch.set_grad_enabled(grad_state)\n",
    "\n",
    "    if return_img_per_step:\n",
    "        return torch.stack(imgs_per_step, dim=0)\n",
    "    else:\n",
    "        return input_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1206fa-f318-450c-b1aa-c98df5b6d9bc",
   "metadata": {},
   "source": [
    "## 4. Setting up Buffer to Store Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8746643-7a3e-4117-871a-951b246b6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "\n",
    "    sample_size = BATCH_SIZE\n",
    "    channels = CHANNELS\n",
    "    img_size = IMAGE_SIZE\n",
    "    device = DEVICE\n",
    "    buffer_size = BUFFER_SIZE\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.examples = [torch.rand(size=(1, self.channels, self.img_size, self.img_size)) * 2 - 1  \n",
    "                             for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps, step_size, noise):\n",
    "        \n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.randn(size=(n_new, self.channels, self.img_size, self.img_size))\n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
    "        input_imgs = torch.cat([old_imgs, rand_imgs], dim=0).detach().to(self.device)\n",
    "\n",
    "        input_imgs = generate_samples(self.model, input_imgs, steps, step_size, noise)\n",
    "        self.examples = list(input_imgs.to(\"cpu\").chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.buffer_size]\n",
    "        \n",
    "        return input_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d90ba-b51e-4320-8af9-e4a45e069bd1",
   "metadata": {},
   "source": [
    "## 5. Setting up Energy-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5d748f91-c81b-4f1c-9cd5-81244b3e075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM:\n",
    "\n",
    "    alpha = ALPHA\n",
    "    lr = LEARNING_RATE\n",
    "    steps = STEPS\n",
    "    step_size = STEP_SIZE\n",
    "    noise = NOISE\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.buffer = Buffer(self.model)\n",
    "\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_step(self, real_imgs):\n",
    "        real_imgs = real_imgs.to(DEVICE)\n",
    "        noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs.add_(noise).clamp_(min=-1.0, max=1.0)\n",
    "        fake_imgs = self.buffer.sample_new_exmps(self.steps, self.step_size, self.noise)\n",
    "        \n",
    "        input_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = self.model(input_imgs).chunk(2, dim=0)\n",
    "\n",
    "        reg_loss = self.alpha * (real_out ** 2 + fake_out ** 2).mean()\n",
    "        cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "        loss = reg_loss + cdiv_loss\n",
    "\n",
    "        print(loss)\n",
    "        return \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "caa91825-b2fb-43c8-b63c-0a8d49501494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Energy().to(DEVICE)\n",
    "ebm = EBM(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a3381f57-8662-443c-84cc-4824a3e3b214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 32, 32])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ced018d-b184-4dbf-90f3-1c9e7e2e9fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0030, device='mps:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ebm.train_step(test_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9951f2e-b9a6-47b1-b84c-ab91dcbbd0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
