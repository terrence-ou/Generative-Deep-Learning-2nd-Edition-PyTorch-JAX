{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebf8f39-66c9-47bb-9ddc-5c2627f4a917",
   "metadata": {},
   "source": [
    "# PixelCNN for FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b06cd-02d6-4b0f-bff2-a1b1a05ced68",
   "metadata": {},
   "source": [
    "### In progress\n",
    "\n",
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393d1206-2d7f-421f-a92c-0a144102c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as Transforms\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f66fa4-5289-4d7c-a5ca-3d95781ab430",
   "metadata": {},
   "source": [
    "## 0. Train Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d2939e-8ba4-46c0-8017-ca4e0a66ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 8\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76640e9c-a3eb-4999-b7e5-b64cb0d8e048",
   "metadata": {},
   "source": [
    "## 1. Preparing FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d27d90e-5f5e-49c7-a768-7f453510359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn input image into (label-pixel representatin, pixel-wise labels)\n",
    "def collate_fn(batch):\n",
    "    batch = torch.stack([data[0] for data in batch])\n",
    "    value_step = 1.0 / PIXEL_LEVELS\n",
    "    labels = (batch / value_step).type(torch.long)\n",
    "    imgs = labels.type(torch.float32) / PIXEL_LEVELS\n",
    "    return imgs, labels\n",
    "\n",
    "def get_dataloader(train=True):\n",
    "    transform_fn = Transforms.Compose([\n",
    "                        Transforms.ToTensor(),\n",
    "                        Transforms.Resize(IMAGE_SIZE, antialias=True),                        \n",
    "                    ])\n",
    "    \n",
    "    # Load FashionMNIST dataset\n",
    "    fashion_ds = datasets.FashionMNIST('../../data', \n",
    "                                      train=train, \n",
    "                                      download=True,\n",
    "                                      transform=transform_fn)\n",
    "\n",
    "    # Get train dataloader\n",
    "    dataloader = DataLoader(fashion_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=8, collate_fn=collate_fn)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee97742-720d-4c6b-bf5b-0c0870553f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check dataset\n",
    "def plot_imgs(batch, num_rows=2, num_cols=6):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for i in range(num_rows * num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        ax.imshow(batch[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "test_loader = get_dataloader()\n",
    "sample_data = next(iter(test_loader))\n",
    "\n",
    "plot_imgs(sample_data[0].permute(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5f380-0074-4947-ba77-02c5a4bf6ae5",
   "metadata": {},
   "source": [
    "## 2. Build the PixelCNN\n",
    "\n",
    "This PyTorch implementation references pi-tau's GitHub repo: [Link](https://github.com/pi-tau/pixelcnn/blob/master/conv2d_mask.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1200c81a-8683-4290-a31a-ac4180f842da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building MaskedConv2D layer\n",
    "class MaskedConv2D(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        kwargs['padding'] = 'same'\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "\n",
    "        assert mask_type in ['A', 'B'], 'Mask type should be either A or B'\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "        # Creating masks\n",
    "        kh, kw = kernel_size\n",
    "        mask = torch.ones_like(self.weight)\n",
    "        mask[:, :, (kh // 2 + 1):, :] = 0\n",
    "        mask[:, :, (kh // 2), (kw // 2 + 1):] = 0\n",
    "        # If mask type is A, then masking the center pixel\n",
    "        if mask_type == 'A':\n",
    "            mask[:, :, (kh // 2), (kw // 2)] = 0\n",
    "\n",
    "        # Making the mask the non-trainable parameter of the module\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight * self.mask, \n",
    "                        stride=self.stride, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e01823c-2bf5-44ee-ab1b-08abaf5192fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"A\" mask of the conv layer:\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "Type \"B\" mask of the conv layer:\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Check the mask in the masked conv layer\n",
    "print(\"Type \\\"A\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('A', 1, 1, 5).mask.numpy())\n",
    "\n",
    "print(\"\\nType \\\"B\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('B', 1, 1, 5).mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb8f94b-00a2-4378-97f0-89df0c676cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # First regular 2D convolution\n",
    "        self.conv_1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=in_channels, \n",
    "                                  out_channels=out_channels // 2,\n",
    "                                  kernel_size=1,\n",
    "                                  stride=1),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        # Type 'B' masked convolution\n",
    "        self.pixel_conv = nn.Sequential(\n",
    "                        MaskedConv2D(\n",
    "                            mask_type='B',\n",
    "                            in_channels=out_channels // 2,\n",
    "                            out_channels=out_channels // 2,\n",
    "                            kernel_size=3,\n",
    "                            padding='same'),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        # Second regular 2D convolution\n",
    "        self.conv_2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=out_channels // 2,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_x = self.conv_1(x)\n",
    "        conv_x = self.pixel_conv(conv_x)\n",
    "        conv_x = self.conv_2(conv_x)\n",
    "        # Skip connection\n",
    "        return conv_x + x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c982afc6-62e3-4688-94d2-d50c833edea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PixelCNN                                 [1, 1, 16, 16, 8]         --\n",
       "├─Sequential: 1-1                        [1, 128, 16, 16]          --\n",
       "│    └─MaskedConv2D: 2-1                 [1, 128, 16, 16]          6,400\n",
       "│    └─ReLU: 2-2                         [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-2                        [1, 128, 16, 16]          --\n",
       "│    └─ResidualBlock: 2-3                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-1              [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-2              [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-3              [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-4                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-4              [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-5              [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-6              [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-5                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-7              [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-8              [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-9              [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-6                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-10             [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-11             [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-12             [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-7                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-13             [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-14             [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-15             [1, 128, 16, 16]          8,320\n",
       "├─Sequential: 1-3                        [1, 128, 16, 16]          --\n",
       "│    └─Sequential: 2-8                   [1, 128, 16, 16]          --\n",
       "│    │    └─MaskedConv2D: 3-16           [1, 128, 16, 16]          16,512\n",
       "│    │    └─ReLU: 3-17                   [1, 128, 16, 16]          --\n",
       "│    └─Sequential: 2-9                   [1, 128, 16, 16]          --\n",
       "│    │    └─MaskedConv2D: 3-18           [1, 128, 16, 16]          16,512\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 16, 16]          --\n",
       "├─Conv2d: 1-4                            [1, 8, 16, 16]            1,032\n",
       "==========================================================================================\n",
       "Total params: 307,976\n",
       "Trainable params: 307,976\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 78.84\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 3.42\n",
       "Params size (MB): 1.23\n",
       "Estimated Total Size (MB): 4.66\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_filters, num_res_blocks, ouput_size=PIXEL_LEVELS):\n",
    "        super().__init__()\n",
    "        self.masked_conv_1 = nn.Sequential( \n",
    "                                MaskedConv2D(\n",
    "                                    mask_type='A',\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=num_filters,\n",
    "                                    kernel_size=7,\n",
    "                                    stride=1,\n",
    "                                    padding='same'),\n",
    "                                nn.ReLU()\n",
    "                             )\n",
    "\n",
    "        self.res_convs = nn.Sequential(*[\n",
    "                            ResidualBlock(\n",
    "                                in_channels=num_filters,\n",
    "                                out_channels=num_filters)\n",
    "                            for _ in range(num_res_blocks)])\n",
    "\n",
    "        self.masked_conv_2 = nn.Sequential(*[\n",
    "                                nn.Sequential(\n",
    "                                    MaskedConv2D(\n",
    "                                        mask_type='B',\n",
    "                                        in_channels=num_filters,\n",
    "                                        out_channels=num_filters,\n",
    "                                        kernel_size=1,\n",
    "                                        padding='valid'),\n",
    "                                    nn.ReLU())\n",
    "                                for _ in range(2)],\n",
    "                            )\n",
    "\n",
    "        self.output_conv = nn.Conv2d(in_channels=num_filters,\n",
    "                                     out_channels=ouput_size,\n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding='valid')\n",
    "        # We don't need a softmax layer when using CrossEntropy Loss in PyTorch\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masked_conv_1(x)\n",
    "        x = self.res_convs(x)\n",
    "        x = self.masked_conv_2(x)\n",
    "        x = self.output_conv(x)\n",
    "        # Manipulate the shape making predictions at the end of the output tensor\n",
    "        x = x.reshape(x.shape[0], 1, PIXEL_LEVELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        x = x.permute(0, 1, 3, 4, 2) \n",
    "        return x\n",
    "\n",
    "pixel_cnn = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS)\n",
    "torchinfo.summary(model=pixel_cnn, input_size=(1, 1, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9093698-308f-46d3-b045-d41e0e2fde53",
   "metadata": {},
   "source": [
    "## 4. Train step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615bc215-f7fd-4112-81aa-d442388baa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator:\n",
    "\n",
    "    def __init__(self, num_imgs):\n",
    "        self.num_imgs = num_imgs\n",
    "\n",
    "    # Sample from the model's output distribution with temperature\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / probs.sum()\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    # Generate new image pixel-by-pixel\n",
    "    def generate(self, model, temperature):\n",
    "        model.eval()\n",
    "        \n",
    "        generated_imgs = np.zeros(shape=(self.num_imgs, 1, IMAGE_SIZE, IMAGE_SIZE))\n",
    "        batch, channels, rows, cols = generated_imgs.shape\n",
    "\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                for channel in range(channels):\n",
    "                    with torch.no_grad():\n",
    "                        probs = model(torch.tensor(generated_imgs, dtype=torch.float32).cuda())[:, :, row, col]\n",
    "                    probs = nn.functional.softmax(probs, dim=-1).squeeze() # We don't have this layer in model\n",
    "                    probs = probs.detach().cpu().numpy()\n",
    "                    generated_imgs[:, channel, row, col] = [\n",
    "                        self.sample_from(x, temperature) for x in probs\n",
    "                    ]\n",
    "                    generated_imgs[:, channel, row, col] /= PIXEL_LEVELS\n",
    "        return generated_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cc8d87-2773-46c8-9173-e74a0deeddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, dataloader, loss_fn, optim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for imgs, targets in dataloader:\n",
    "        imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
    "        optim.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, PIXEL_LEVELS), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validation(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in dataloader:\n",
    "            imgs, targets = imgs.to(DEVICE), targets.to(DEVICE)\n",
    "            logits = model(imgs)   \n",
    "            loss = loss_fn(logits.reshape(-1, PIXEL_LEVELS), targets.reshape(-1))\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    return valid_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a103523-b6f8-4b1e-a1ba-43e65e03799a",
   "metadata": {},
   "source": [
    "## 3. Define the model, dataloader, objective, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e468fb7-901c-46e2-84d9-d521e899f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS).to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(pixel_cnn.parameters(), lr=5e-4)\n",
    "\n",
    "image_generator = ImageGenerator(12)\n",
    "\n",
    "train_loader = get_dataloader(train=True)\n",
    "valid_loader = get_dataloader(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c7a90-7220-4b6f-ae86-65bcc699775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1\tTime:0.09 min\n",
      "\tTrain loss: 0.6961  Valid loss: 0.5874\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    prev_time = time.time()\n",
    "    \n",
    "    train_loss = trainer(pixel_cnn, train_loader, loss_fn, optim)\n",
    "    valid_loss = validation(pixel_cnn, valid_loader, loss_fn)\n",
    "    \n",
    "    curr_time = time.time()\n",
    "\n",
    "    if i == 0 or (i + 1) % 5 == 0:\n",
    "        print(f'Epoch {i+1:3d}\\tTime:{(curr_time - prev_time) / 60:.2f} min')\n",
    "        print(f'\\tTrain loss: {train_loss:.4f}  Valid loss: {valid_loss:.4f}')\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        generated_imgs = image_generator.generate(pixel_cnn, 1.0)\n",
    "        generated_imgs = np.transpose(generated_imgs, (0, 2, 3, 1))\n",
    "        plot_imgs(generated_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8227496-8fda-4050-b511-3f33eebefed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_imgs = image_generator.generate(pixel_cnn, 1.0)\n",
    "generated_imgs = np.transpose(generated_imgs, (0, 2, 3, 1))\n",
    "plot_imgs(generated_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358949c7-4d08-4fac-965d-c408142cbd75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
