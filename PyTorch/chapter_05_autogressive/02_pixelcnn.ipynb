{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebf8f39-66c9-47bb-9ddc-5c2627f4a917",
   "metadata": {},
   "source": [
    "# PixelCNN for FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b06cd-02d6-4b0f-bff2-a1b1a05ced68",
   "metadata": {},
   "source": [
    "### In progress\n",
    "\n",
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393d1206-2d7f-421f-a92c-0a144102c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as Transforms\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f66fa4-5289-4d7c-a5ca-3d95781ab430",
   "metadata": {},
   "source": [
    "## 0. Train Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d2939e-8ba4-46c0-8017-ca4e0a66ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 12\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76640e9c-a3eb-4999-b7e5-b64cb0d8e048",
   "metadata": {},
   "source": [
    "## 1. Preparing FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d27d90e-5f5e-49c7-a768-7f453510359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn input image into (label-pixel representatin, pixel-wise labels)\n",
    "# def collate_fn(batch):\n",
    "#     batch = torch.stack([data[0] for data in batch])\n",
    "#     value_step = 1.0 / PIXEL_LEVELS\n",
    "#     labels = (batch / value_step).type(torch.long)\n",
    "#     imgs = labels.type(torch.float32) / PIXEL_LEVELS\n",
    "#     return imgs, labels\n",
    "\n",
    "def get_dataloader():\n",
    "    transform_fn = Transforms.Compose([\n",
    "                        Transforms.ToTensor(),\n",
    "                        Transforms.Resize(IMAGE_SIZE, antialias=True),                        \n",
    "                    ])\n",
    "    \n",
    "    # Load FashionMNIST dataset\n",
    "    fashion_ds = datasets.FashionMNIST('../../data', \n",
    "                                      train=True, \n",
    "                                      download=True,\n",
    "                                      transform=transform_fn)\n",
    "\n",
    "    # Get train dataloader\n",
    "    train_loader = DataLoader(fashion_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=8)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee97742-720d-4c6b-bf5b-0c0870553f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAACxCAYAAADXnPd8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAceUlEQVR4nO3dWYzdZf3H8e8AZSmUgZlO6TpdbUs3oCyC004kglVZFBUkDSqJGkAuMMHlwsSYghdqNKbGkEqIW6JyoUmBGilLqwFbSovdmNLp3k7pNt07bWmBevG/+Pt8v5/OPHY6y3n6ft09X545/c3v/M5vHs75nO9TderUKQMAACjBeT19AAAAAGcLCxsAAFAMFjYAAKAYLGwAAEAxWNgAAIBisLABAADFuKC9/1hVVdXhd8EvuuiiUBs6dGioDRo0KBlfc801Yc7tt98eah988EEyvuCCeMj+sc3Mli1bFmqvvvpqMl61alWYs3fv3lDbs2dPqHmnTp2q6nDS/yjn/OP/9NT5P++8+P8GtbW1ofbRj340GU+bNi3Mueuuu0Jt7ty5ybhPnz5hzr333htq69atC7WXX345Gc+bNy/MaWpqCrUPP/ww1Lyzff659vNx7+lZlXTtDx48ONSefPLJUKurq0vGzc3NYc748eND7YUXXgi1p59+Ohm///77HR5nrtOde96xAQAAxWBhAwAAisHCBgAAFKOqvS0Vcj7ru+yyy0LtgQceCLXGxsZkfPHFF4c5Kj/gP8cbNWpUmOOzM2ZmbW1tofbee+8l4x07doQ5f/vb30Jt/vz5oebxOXfP6qnzP3Xq1FD78Y9/HGrXXnttMq6urs46hpMnTybj888/P8xROTeVi/G1NWvWhDmPPfZYqC1YsKDD46yknEFpuPf0rEq69qdMmRJqzz//fKgNHDgwGV944YVhzpEjR0JN/f386le/moyPHz/e4XHmImMDAACKx8IGAAAUg4UNAAAoRrt9bHKoz9n+/Oc/h5rvBaN61qi8z9q1a5PxsWPHwhyVMVDZn127diXjJUuWhDn/+Mc/Qg04na997WuhdsMNN4Sa77/09ttvhzmtra2h5j+PVp91nzhxItT69esXahMmTEjGw4YNC3O++c1vhtrrr7/e4b8H9AT1eujbt2+oHTx4MBm3ly0tmcq3qL+p+/fv7/CxVN7v0KFDoZbTB+ts4x0bAABQDBY2AACgGCxsAABAMVjYAACAYnQ6PKz4oJaZ2aJFi5LxjTfeGOY0NDSE2vLly5PxgQMHwpwZM2aE2ltvvRVqS5cuTcZ+U0Cz2BAN+G8DBgxIxqrhVQ4fijfTm636TTZVMNJvFGtmtmHDhlAbOXJkh8c1adKkUBsxYkQyVhvilciHI9U9q76+PtTUvefw4cPJ2DcLNdNNS/2Ggf3799cHew5QG86qL6F86UtfCrWdO3cm47/+9a9hzptvvhlq6rVVyY4ePZpV89dZVVXsg6eej40bN4Ya4WEAAIBOYGEDAACKwcIGAAAUg4UNAAAoRpeEh1XQyBs+fHioDR06NNT8rshq127176mdu32XZB/MQ+TP7bnUsVNdV7fccksyHjNmTJijOmH7EOLll18e5qjwsA/eXXrppR0+tpnZ9u3bQ82H+lV34kGDBoXauHHjkvG5Eh7250d1Zf7sZz8baure459bH2Y1052g/fP4wgsv6IM9B6hr//777w+16dOnh1pdXV0yvvXWW8Ocn//856E2b968ZOxD4JVG/f1UNR+czw0P7969O9QIDwMAAHQCCxsAAFAMFjYAAKAYXZKxyaHyBKpRkN+lW+Vi1C6j1dXVoeY/EzyX8iKe2hV36tSpoebP9+rVq8MctWNsDr/jtZn+LNd/RttdTbP69OkTar4hn/r8WP3cJZdc0u7YTJ8Pf43u3bs3zFGfkdfU1HR4DKohnNq5+xOf+EQyfvHFF8OcEvmMzfXXXx/mqDyVaoToM4Uqn6Cum5aWlmQ8a9YsfbAF8tfnww8/HOZMnjw567G2bt2ajGtra8OcH/3oR6H2mc98psM5lUTdO3MyMOq+nHOv7im8YwMAAIrBwgYAABSDhQ0AACgGCxsAAFCMLgkPq1Cub0imwqt/+tOfQm3ixInJ2DdIMzNbuHBhqG3evDnUfIMnFTouYTdXFUz0jdcefPDBMOdzn/tcqPnw8O9///swZ8GCBaGmAsV+52i/a7SZblK2f//+ZKwaaXUFFaT9xS9+kYznz58f5txxxx2h5oOPqtnYwIEDQ82/ltRrS+0Ura5tf134YKqZ2dy5c0Pt2WefTcbnSmNL/5rxO7ub6dea4oOWucF5/4WKLVu2ZP17lcZ/ScTM7NFHH03G6v6k7iGq+aXf7V41UVTX9U033ZSMVTC8klx55ZWhpr5o4OV+0UY12fWvke4IGPOODQAAKAYLGwAAUAwWNgAAoBgsbAAAQDG6LTx8xRVXJGPVsVOFIBctWpSMly9fnvXvqSBe3759k7EK8KlgU091KFYhuMGDByfjq6++OswZMmRIqDU0NCRjH+Q10+ffh/p++MMfhjmPP/54qL3xxhuhtmLFimSsOq2qXan9Y/VkeNXv7qt+T9/l1CyG3lUXWxVM9GF2dV2r61PVfAh75syZYU5TU1OoqRB1afy9wczsy1/+cjJWAdfcc6+etzNRiV9u8Pd69QWBr3/966HW2NiYjNX5V9emuj+MGjWqw+NUXzjxz6UKx1YSdR7q6+tDzf/eudev/ztjFu/z3XH/5h0bAABQDBY2AACgGCxsAABAMbptd2+/s7D6rFjlbvxnfSoHoj6zUw3Q/DGoJma9acdv9dnnQw89lIxVJuLuu+8ONf+5dmtra5jzgx/8INR8pkTtAO4baZnp/Ib/fFc19nv99ddDze9orRrZ9SbqGj148GAyzt0Z9+TJk8n4TBvCmcXX3L59+8KccyFPo+4NjzzySKj5jE1ndjj289R9Rv2cv0dVV1eHOd3FN1VV16I6t/fcc08y/vznPx/m+J3UzWKTT3Wu1flQ93X/s+reqv4m+d+n0jM2V111Vaipc+/v+4q6hkePHh1qfpf2Y8eOdfjYncU7NgAAoBgsbAAAQDFY2AAAgGKwsAEAAMXotvCwb4CVE7BTcoJhZrr5ng9JqTm9iQri+UaHqtGe2nH63nvvTcYrV67s8LFVzYdgzcx+9rOfhZoPjJnF8KF63vxu7mZm48ePT8a9fYdj9Xup6zbn53LDwp4K9vkQtg8ml0A1tZwwYUIy/sY3vhHm3HfffaHm7w+5gd+c+5gKqqrH9/cs9froCqrJ5yc/+clk7F+XZmZjx44Ntdra2mSsdklXz1tzc3My9s1Jzcy2bdsWaqrxZ//+/UPNU40D/RcBejK8fSb8taju8Tn3ndwv1ai/Wf6c+UahXYF3bAAAQDFY2AAAgGKwsAEAAMVgYQMAAIrRJenZnI6UqluwCvP60KUK6+XyQTwVMutN1qxZE2pz5sxJxirkpzpJzp49OxmrMGtdXV2o+XnquVVdKnOCqeqxVJDNh81U8Lk3UR20fbdkFR7N6Tyr5IaA/fWurv/c3cO72q233hpq999/fzJWYVLVWdV3vFbPz5me+9xuxDm7Jed0UFe/X1f44he/GGozZsxIxrt27Qpz1A7cvsvv8OHDwxz1evBfODl69GiYM2XKlFBTz68/376rsZnZkSNHQs3P646dqc8m/3v77vtmea/v3A7b6p6iAstdjXdsAABAMVjYAACAYrCwAQAAxWBhAwAAitHp8LAKFanugyNHjkzGJ06ciAcjwsM5nVfVnJzHUiFb3521Jx06dCjUFi1alIxVUC4nIKZCcLkBMU8F/xT/+LkBWl/rTQG+3C7DvhNzbsfanNBpLh/sq6mpCXNaWlo6PIbu8NBDD4VaY2NjMs4NJeYcvwo95gTnVQjVd6s1i/c7FbhXHVl3796djDds2BDmdIW2trYOj0UFmdW139TUlIw3btwY5vigsFl83tRzpF5HKkzv56mgsHp+/e+Te6/rLfz9Qv1dPNPfKTc4r7rQdzXesQEAAMVgYQMAAIrBwgYAABSj0xkb9fn1mTYoO9OGebm7e/vj8sdkpneO7omMwen486aaVqkaukbujs8+i9GZrIynsgHqGPzu6rk7/faE73//+6HW0NDQ7tjMbNCgQaHmd47258FM34985mX79u1hzurVq0Nt1apVodba2pqMVcZQZcf88+hzLl3l2WefDbWlS5cm42nTpoU5audufy9W51o19vPnSGUHFZV78q8RdZ2r+6Z/TlasWJF1DL2Vyrvk/H1T9xj1PKp5aj3Q1XjHBgAAFIOFDQAAKAYLGwAAUAwWNgAAoBhdsru3Cgv5mgp4qZ/zQT/V/Eo1hVI1H2JTDaZ6U1AYlSmnaZgK2alrz9dyf06FI/1rZ8+ePVmP1RNUIzpf+8Mf/hDmqICpD0yqe4P6vX2YtDOhfP985O5s72vd1ZxSNRBcvnx5u2OzvJ21FXU+/O+as9u6mX79+cdXz7cKw+Y0Nu3N/O9dXV3dpf+eeq5V88Wuxjs2AACgGCxsAABAMVjYAACAYrCwAQAAxeiS8PDo0aNDzYfKVMhIdQT1u7DmdixWj+WDVH7H8dP9nOoSCpyOChgePHgwGavrP2cne3V9qpoK5/vXSU90BD2bcgK/p6t1N3+slbZLdC715Q70HH+fqa2tDXPU6+hMdwFQO6vn3NfONt6xAQAAxWBhAwAAisHCBgAAFKNLMjYtLS2htnDhwmRcU1MT5tTX14eazyuoz3BVnkB9rr5jx45kvHXr1jAH6CyVsVm/fn0yzmkiZhY/61bZHJXXaGtrCzX/syqbA6Ac/t7wxhtvhDnjxo0LNd/IT92bVPb03XffDbUtW7Z0eJxnG+/YAACAYrCwAQAAxWBhAwAAisHCBgAAFKOqt+zmCwAA0Fm8YwMAAIrBwgYAABSDhQ0AACgGCxsAAFAMFjYAAKAYLGwAAEAxWNgAAIBisLABAADFYGEDAACKwcIGAAAUg4UNAAAoBgsbAABQDBY2AACgGCxsAABAMS5o7z9WVVWd6q4DqXSnTp2qOtuPyfnP19vP/7Rp05Lxo48+GubU19eH2vr165Px1KlTs/69BQsWhNoTTzyRjPfs2ZP1WDnO9vnn2s/X26/90lXStX/BBfFP/ogRI0LtuuuuS8abNm0Kc+rq6kLtlVdeCbWTJ092eFynTp3Zr3y6c887NgAAoBgsbAAAQDFY2AAAgGK0m7EB8L9Tn2NPnjw5GV9++eVhTk1NTajdeeedyfj9998Pcy666KJQmzlzZqj5z7/nzp0b5gCoTH369Am1sWPHJmOV0bvvvvtCzd93Dh06FOZ8+OGHoTZ79uxQmz9/fjJesWJFmNPW1hZqZ5q7MeMdGwAAUBAWNgAAoBgsbAAAQDEqLmNTVZXXMmD69Omhdv311ydjlWlQn1MeO3YsGc+bNy/rGHBuuvjii0Nt0qRJydj3iTAzu+SSS0LtnXfeScYqv3PVVVeFmpo3ePDgZKxeS535XLs0552X/n+fykVddtllobZv375Qy7lvXXjhhaF25MiRZJzTE6S3U79n3759Q82ff5Xp8PdmM7P33nuvE0dXuXyvLDOzb3/728l4zJgxYc7KlStDbffu3cl4//79YU51dXWoPfDAA6HW0NCQjJ9++ukw5y9/+UuoqTxhLt6xAQAAxWBhAwAAisHCBgAAFIOFDQAAKEbFhYdVuFFtHvi9730v1HwwuLa2NszxAUuz2ADNB6uA/3b++eeHmg9M9uvXL8zZuHFjqE2cODEZb9myJcw5fvx4qA0cODDU/L+pQpznavBS8cHgj3/842GOCkuqe5Q/12vXrg1z1H3lzTffTMavvfaaPNbu4APQqjGkD8mbmd14443JeOjQoWGOCs57Kjx8+PDhUNu5c2eovf3228m4qakpzFEB2d4apvfBajOzu+66K9R8g77hw4eHORs2bAi1f//738l40KBBYc7SpUtDTYXb/X3ntttuC3MWLlwYart27Qq1XLxjAwAAisHCBgAAFIOFDQAAKAYLGwAAUIyKCw+rDp7XXHNNqKmA2qZNm5LxgAEDsh5/7969ydh3gz2XqNCa6tZ84sSJUDvTIF5OB9LeRJ0P/zscOHAgzFE73C5btiwZq3D7FVdcEWoqnOw7IqvuxOdqeFi97v3zqJ5X9XzccMMNHT7Wc889F+YcPXo01FS34+6gXuf+WL7yla+EOarju7/O1H1A/Xu+poKpqjutCjB/+tOfTsbNzc1hzq9//etQ88Ha3homNtPn5913303G6r68ffv2UBsyZEgyVrt7+07+ZmYvvfRSqE2ePDkZq8C36p5OeBgAAMBY2AAAgIKwsAEAAMWouIyN/+zPzOyRRx4JNZVh8E2yVIMp1bTM5xVUY79Kk7uzsz8f99xzT5jzhS98IdR+8pOfhJrPi6idkSdMmBBqvuHdkiVLwpzeRDX6euKJJ5Kx2gH6Yx/7WKj5a03lMNRz+cwzz4TaU089lYxVpgf/zzclU7sZq2yA4l9bKg/xwQcfhJq/9nN2CT8b1OvwscceS8aqMar6Hfw9RGWVVLbLP5a6X6vnRJ1b//jjxo0Lcx5//PFQmzNnTjJWO2H3hLq6ulCrqakJNf97q7+Lt99+e6j9/e9/T8bq767PnprpbJ9vPKmyfTfddFOodeZc844NAAAoBgsbAABQDBY2AACgGCxsAABAMXp9eHjEiBHJeNasWWGO2nnUNyYyiwGoK6+8MsxRATUfdhs1apQ81kqifs8pU6aEmg9m33333WGO2s16zJgxoeYDtH7XXzOzhx9+ONTmzp2bjN96660wpzdRAcqWlpZkrJpPqVClr6nHVjsjL168ONRaW1vjwcLM9L3AN/5U57lv376hpprN+dCvmqOaoPnmi77JYlfxDe3MzMaPH5+MVXhUBVj9eVO/g/rigj9n6j6jaqqBp2/kd/z48TBHvf58YPpb3/pWmNMTVABX/V0aOXJkMlbhXnUt+vvMpZdeGub4HdPN9JdvfE09ZyoM3Rm8YwMAAIrBwgYAABSDhQ0AACgGCxsAAFCMXhUeHj58eKj98pe/TMYTJ04Mc3bs2BFqaldc3+n22LFjYU5OONOH6LqK6qrpO9GqsJYKg/nOjqqDcGNjY6j58+i7SJrpsJ4KD//mN78JNU911vWPpcJnlUaFt33Qz8xs3bp1HT6WCp2qEKe/LtS1fi5Qrxm1U7G/9tXr6kx3e1a7UquaD7TmdjruLHUsnjoWFbD21HlU9xAfkM3ZAdxMd2f2j6VC36q2bdu2ZKy6GvcE9bdB7TTv7x+qw7P6m+r/xqku8eraV7uH+3vRsGHDwpzVq1eHmr/2/5dzzzs2AACgGCxsAABAMVjYAACAYnRbxsZ/7qka/jz44IOhdu211yZjla9Qj6U+s/XHoD5HVp/j+aZG6vPGrvDkk0+G2qc+9alkrD4XVhkCnxdQc1TTON/oUDWGUnkRdW79c6LyNOpz2/79+yfj7soZnE0+C6A+I1dZmY985CPJ+ODBg2FOzo7K5zJ/7lUjM7V7sc8L+OvQTDebU/eeNWvWJONVq1Z1+O+ZxXuWum66groW/e+qmtypWs61qO4rKj/jqXOdk4VS93l1DL7WXburd8Q3bjSL+Uszs3/+85/JWN0rchqDqr+x77zzTqipJq/+3zx8+HCY4xvxmsXX5ObNm8Oc0+EdGwAAUAwWNgAAoBgsbAAAQDFY2AAAgGK0Gx5WQSkf3lU7a6uAkm+ydvPNN4c5t9xyS6j5MJoKoqlAXU7IVT2WajDl53XXDrtqh3IfglONCJUjR450OEc93z40poJm6pzlNNFTIWwVKPbnf8aMGR0+dm/jz606Z21tbaHmr4GBAweGOWpn3KFDh3Z4DCVSv+OAAQOSsWqwqRqX5QTuX3vttVBTux7v3LkzGedc52bxXpMTqD0b/PGaxS8EqPu8ujeeaQDX3+vUz6laTgNA9XPqCw+TJk1Kxqq5XE9Qf/M2btwYaj5k3NTUFOb4XezN4v1J3ZvUvUiF8P/1r38lYxWSVw0yO/N3lndsAABAMVjYAACAYrCwAQAAxWBhAwAAitFueFjtNvyd73wnGavQj+oe64OiPtBnltd5UwXWcrtW+poKJ6ljb2lpScbd1dX1pz/9aag9//zzyVgFaVUIe+zYsclYnX/VQdg/J1u3bg1zmpubQ82fM7O4m7oKNKvdw7ds2ZKM169fH+ZUGtUxVXUV9uF8dV2rXerV7t6VHh72gXTVDVVd11dffXUyVsFLdV79fUV11V26dGmoqXPvn8cDBw5k/ZwPbap7XVdQAWh/b1Rdz9U91d9X1HWo7rs54WH1JQV1f/bHkLMLuZnZjh07krH6m9gT1P1DhXk3bdqUjG+77bYwR4WmfXd3FUxWryMVMvZdrH03dTMd3u9MUJ53bAAAQDFY2AAAgGKwsAEAAMVgYQMAAIrRbhLtu9/9bqjNnDkzGasOlSoE6bcqV4FHFV71ASLV+Vb9nA8/mcVg6h//+Mcwx4dszcwWL16cjF999dUwpyu64apQ1+rVq5Ox6iT5q1/9KtR80E91LFbn0XdIPXnyZJiT2yHSB5FzOoSaxcDkvn37sv693sQHH1XHbhX+88+J6lib2/m5ksLDqpvy6NGjk7F6jecEWtV13q9fv1Dz81RYUr1GX3755VBbt25dMp4+fXqYo659/9zmhl47q3///qHmw7z19fVhTk7HdzVH/e4+YK2+XJLLB4pV1/NXXnkl1Pw1pl6jPUGdiz179oTarl27kvF1110X5qgvcfgvbKiwu7oW1TXhX1vqiyXqNamuiVy8YwMAAIrBwgYAABSDhQ0AAChGuxmb/fv3h5rPz6iGWKrRlG+spxoyqc/UfC5A5QRUU6Dnnnsu1GbPnp2MVXMt9Tm6/51Vo66eos6ZOj5fy82p5Jz/rt5x2P+Onfnstaf46119rq125PafkasGlep1qpqUVVLGxu+qbBZ35VbZLpWx8deLynjkZGxU07oXX3wx1PxzZmZ25513JmOVYVH5NU/lirpCQ0NDqPnmdLnXsM97qfyXuq7986bmqHOmjmvZsmXJWGUTJ0+eHGo+QzV48OAwpyf4XbvNdP60tbU1GatspWqO5zOwEyZMCHPU86h2u/d5IPVY/jjN9N/1XLxjAwAAisHCBgAAFIOFDQAAKAYLGwAAUIx2w8OzZs0Ktd/+9rfJWIXgVHi4trY2GavgnwqF+mCT2v1527ZtobZ58+ZQ82EkFWDuTcHg3sCfo9zQN1I+uKvCfyp0OmzYsGSsmnCpkJ1qgFlJfNjTLDa5GzJkSJij7j0+MKnuPep8+YDphg0bsn5OHYNvOKhCrzlfjFANAbvC7373u1DzjQdVwFs1nvTnX4W31bXvg6jqd1eB4pzmdaqRXGNjY6j5L5P4a7CnqONQf7t8KFf9Xdy+fXuo5TRFVU0mVaM9v5u33+3bzGzv3r2hpp7bXLxjAwAAisHCBgAAFIOFDQAAKAYLGwAAUIx2w8MqGLd27dp2xwAiHwxVAVYVjPehStUZWwX2VDi5kqiAoa+pXYJ7AxW0fOqpp3rgSM6c2s3Z11Qn5kqhjr2Sfh/1JQ71JQIfmlYB45tvvjnUfHBXdQZWHYtV92PfBb25uTnMUYH7zuxkzzs2AACgGCxsAABAMVjYAACAYrSbsQFwdvjmYosWLQpz7rjjjg4fRzWj9M0vzcwWL17c4TEAqEwqAzVnzpxQW7JkSTIeO3ZsmHP48OFQ87k9lXlbuXJlqKnsjz+Gl156KcxRzR5VricX79gAAIBisLABAADFYGEDAACKwcIGAAAUo0qFfQAAACoR79gAAIBisLABAADFYGEDAACKwcIGAAAUg4UNAAAoBgsbAABQjP8AQRawolEMHrYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x216 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check dataset\n",
    "def plot_imgs(batch, num_rows=2, num_cols=6):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for i in range(num_rows * num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        ax.imshow(batch[i].permute(1, 2, 0), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "test_loader = get_dataloader()\n",
    "sample_data = next(iter(test_loader))\n",
    "\n",
    "plot_imgs(sample_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5f380-0074-4947-ba77-02c5a4bf6ae5",
   "metadata": {},
   "source": [
    "## 2. Build the PixelCNN\n",
    "\n",
    "This PyTorch implementation references pi-tau's GitHub repo: [Link](https://github.com/pi-tau/pixelcnn/blob/master/conv2d_mask.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1200c81a-8683-4290-a31a-ac4180f842da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building MaskedConv2D layer\n",
    "class MaskedConv2D(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        kwargs['padding'] = 'same'\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "\n",
    "        assert mask_type in ['A', 'B'], 'Mask type should be either A or B'\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "        # Creating masks\n",
    "        kh, kw = kernel_size\n",
    "        mask = torch.ones_like(self.weight)\n",
    "        mask[:, :, (kh // 2 + 1):, :] = 0\n",
    "        mask[:, :, (kh // 2), (kw // 2 + 1):] = 0\n",
    "        # If mask type is A, then masking the center pixel\n",
    "        if mask_type == 'A':\n",
    "            mask[:, :, (kh // 2), (kw // 2)] = 0\n",
    "\n",
    "        # Making the mask the non-trainable parameter of the module\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.weight * self.mask, \n",
    "                        stride=self.stride, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e01823c-2bf5-44ee-ab1b-08abaf5192fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"A\" mask of the conv layer:\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "Type \"B\" mask of the conv layer:\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Check the mask in the masked conv layer\n",
    "print(\"Type \\\"A\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('A', 1, 1, 5).mask.numpy())\n",
    "\n",
    "print(\"\\nType \\\"B\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('B', 1, 1, 5).mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb8f94b-00a2-4378-97f0-89df0c676cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=in_channels, \n",
    "                                  out_channels=out_channels // 2,\n",
    "                                  kernel_size=1,\n",
    "                                  stride=1),\n",
    "                        nn.ReLU(inplace=True))\n",
    "\n",
    "        self.pixel_conv = nn.Sequential(\n",
    "                        MaskedConv2D(\n",
    "                            mask_type='B',\n",
    "                            in_channels=out_channels // 2,\n",
    "                            out_channels=out_channels // 2,\n",
    "                            kernel_size=3,\n",
    "                            padding='same'),\n",
    "                        nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=out_channels // 2,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=1),\n",
    "                        nn.ReLU(inplace=True))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_x = self.conv_1(x)\n",
    "        conv_x = self.pixel_conv(conv_x)\n",
    "        conv_x = self.conv_2(conv_x)\n",
    "        return conv_x + x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6023b96-00df-4fa5-877b-abe629115406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the residual block\n",
    "# model = ResidualBlock(4, 4)\n",
    "# torchinfo.summary(model=model, input_size=(1, 4, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c982afc6-62e3-4688-94d2-d50c833edea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_filters, num_res_blocks, ouput_size=PIXEL_LEVELS):\n",
    "        super().__init__()\n",
    "        self.masked_conv_1 = nn.Sequential( \n",
    "                                MaskedConv2D(\n",
    "                                    mask_type='A',\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=num_filters,\n",
    "                                    kernel_size=7,\n",
    "                                    stride=1,\n",
    "                                    padding=3),\n",
    "                                nn.ReLU(inplace=True)\n",
    "                             )\n",
    "\n",
    "        self.res_convs = nn.Sequential(*[\n",
    "                            ResidualBlock(\n",
    "                                in_channels=num_filters,\n",
    "                                out_channels=num_filters)\n",
    "                            for _ in range(num_res_blocks)])\n",
    "\n",
    "        self.masked_conv_2 = nn.Sequential(*[\n",
    "                                nn.Sequential(\n",
    "                                    MaskedConv2D(\n",
    "                                        mask_type='B',\n",
    "                                        in_channels=num_filters,\n",
    "                                        out_channels=num_filters,\n",
    "                                        kernel_size=1,\n",
    "                                        padding='valid'),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "                                for _ in range(2)],\n",
    "                            )\n",
    "\n",
    "        self.output_conv = nn.Conv2d(in_channels=num_filters,\n",
    "                                     out_channels=ouput_size,\n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding='valid')\n",
    "        # We don't need a softmax layer when using CrossEntropy Loss in PyTorch\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masked_conv_1(x)\n",
    "        x = self.res_convs(x)\n",
    "        x = self.masked_conv_2(x)\n",
    "        x = self.output_conv(x)\n",
    "        x = x.reshape(len(x), 1, PIXEL_LEVELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        x = x.permute(0, 2, 1, 3, 4) \n",
    "        return x\n",
    "\n",
    "# model = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS)\n",
    "# torchinfo.summary(model=model, input_size=(1, 1, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a103523-b6f8-4b1e-a1ba-43e65e03799a",
   "metadata": {},
   "source": [
    "## 3. Define the model, dataloader, objective, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e468fb7-901c-46e2-84d9-d521e899f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS).to(DEVICE)\n",
    "\n",
    "# if torch.__version__.split('.')[0] == '2':\n",
    "#     torch.set_float32_matmul_precision('high')\n",
    "#     # It is important to use eager backend here to avoid\n",
    "#     # distribution mismatch in training and predicting\n",
    "#     pixel_cnn = torch.compile(pixel_cnn, dynamic=True)\n",
    "#     print('model compiled')\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(pixel_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9093698-308f-46d3-b045-d41e0e2fde53",
   "metadata": {},
   "source": [
    "## 4. Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "615bc215-f7fd-4112-81aa-d442388baa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator:\n",
    "\n",
    "    def __init__(self, num_imgs):\n",
    "        self.num_imgs = num_imgs\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def generate(self, temperature):\n",
    "        model.eval()\n",
    "        \n",
    "        generated_imgs = np.zeros(shape=(self.num_imgs, 1, IMAGE_SIZE, IMAGE_SIZE))\n",
    "        batch, channels, rows, cols = generated_imgs.shape\n",
    "\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                for channel in range(channels):\n",
    "                    with torch.no_grad():\n",
    "                        probs = model(torch.tensor(generated_imgs, dtype=torch.float32).cuda())[:, :, row, col]\n",
    "                    probs = nn.functional.softmax(probs, dim=-1).squeeze()\n",
    "                    probs = probs.detach().cpu().numpy()\n",
    "                    generated_imgs[:, channel, row, col] = [\n",
    "                        self.sample_from(x, temperature) for x in probs\n",
    "                    ]\n",
    "                    generated_imgs[:, channel, row, col] /= PIXEL_LEVELS\n",
    "        return generated_imgs\n",
    "\n",
    "image_generator = ImageGenerator(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7cc8d87-2773-46c8-9173-e74a0deeddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, dataloader, loss_fn, optim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for imgs, _ in dataloader:\n",
    "        # imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        optim.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        \n",
    "        # logits = logits.reshape(-1, PIXEL_LEVELS)\n",
    "        # targets = (imgs.reshape(-1) * PIXEL_LEVELS).long()\n",
    "        targets = (imgs * PIXEL_LEVELS).long()\n",
    "        \n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77c7a90-7220-4b6f-ae86-65bcc699775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7465089253107706\n",
      "0.6187924376169841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# if i % 20 == 0:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#     generated_imgs = image_generator.generate(1.0)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#     plt.figure(figsize=(5, 5))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#         ax.axis('off')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model, dataloader, loss_fn, optim)\u001b[0m\n\u001b[1;32m      7\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      8\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# logits = logits.reshape(-1, PIXEL_LEVELS)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# targets = (imgs.reshape(-1) * PIXEL_LEVELS).long()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m targets \u001b[38;5;241m=\u001b[39m (imgs \u001b[38;5;241m*\u001b[39m PIXEL_LEVELS)\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m, in \u001b[0;36mPixelCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_conv_1(x)\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_convs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_conv_2(x)\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m conv_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_conv(conv_x)\n\u001b[1;32m     33\u001b[0m conv_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_2(conv_x)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_x\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    loss = trainer(pixel_cnn, train_loader, loss_fn, optim)\n",
    "    \n",
    "    # if i % 20 == 0:\n",
    "    #     generated_imgs = image_generator.generate(1.0)\n",
    "    #     plt.figure(figsize=(5, 5))\n",
    "    #     for i in range(4):\n",
    "    #         ax = plt.subplot(2, 2, i+1)\n",
    "    #         ax.imshow(generated_imgs[i][0], cmap='gray')\n",
    "    #         ax.axis('off')\n",
    "    # plt.show()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc40ed2-6503-4de7-a534-1ad9326f9d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
