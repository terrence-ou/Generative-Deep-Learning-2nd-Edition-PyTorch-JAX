{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebf8f39-66c9-47bb-9ddc-5c2627f4a917",
   "metadata": {},
   "source": [
    "# PixelCNN for FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b06cd-02d6-4b0f-bff2-a1b1a05ced68",
   "metadata": {},
   "source": [
    "### In progress\n",
    "\n",
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393d1206-2d7f-421f-a92c-0a144102c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as Transforms\n",
    "import torchvision.utils as vuitls\n",
    "\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f66fa4-5289-4d7c-a5ca-3d95781ab430",
   "metadata": {},
   "source": [
    "## 0. Train Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d2939e-8ba4-46c0-8017-ca4e0a66ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 12\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76640e9c-a3eb-4999-b7e5-b64cb0d8e048",
   "metadata": {},
   "source": [
    "## 1. Preparing FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d27d90e-5f5e-49c7-a768-7f453510359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn input image into (label-pixel representatin, pixel-wise labels)\n",
    "def collate_fn(batch):\n",
    "    batch = torch.stack([data[0] for data in batch])\n",
    "    value_step = 1.0 / PIXEL_LEVELS\n",
    "    labels = (batch / value_step).type(torch.int)\n",
    "    imgs = labels.type(torch.float32) * PIXEL_LEVELS\n",
    "    return imgs, labels\n",
    "\n",
    "transform_fn = Transforms.Compose([\n",
    "                    Transforms.ToTensor(),\n",
    "                    Transforms.Resize(IMAGE_SIZE, antialias=True),                        \n",
    "                ])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_ds = datasets.FashionMNIST('../../data', \n",
    "                                  train=True, \n",
    "                                  download=True,\n",
    "                                  transform=transform_fn)\n",
    "\n",
    "# Get train dataloader\n",
    "train_loader = DataLoader(fashion_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee97742-720d-4c6b-bf5b-0c0870553f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check dataset\n",
    "def plot_imgs(batch, num_rows=2, num_cols=6):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for i in range(num_rows * num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        ax.imshow(batch[i].permute(1, 2, 0), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "sample_data = next(iter(train_loader))\n",
    "plot_imgs(sample_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5f380-0074-4947-ba77-02c5a4bf6ae5",
   "metadata": {},
   "source": [
    "## 2. Build the PixelCNN\n",
    "\n",
    "This PyTorch implementation references pi-tau's GitHub repo: [Link](https://github.com/pi-tau/pixelcnn/blob/master/conv2d_mask.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1200c81a-8683-4290-a31a-ac4180f842da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building MaskedConv2D layer\n",
    "class MaskedConv2D(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        kwargs['padding'] = 'same'\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "\n",
    "        assert mask_type in ['A', 'B'], 'Mask type should be either A or B'\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "        # Creating masks\n",
    "        kh, kw = kernel_size\n",
    "        mask = torch.ones_like(self.weight)\n",
    "        mask[:, :, (kh // 2 + 1):, :] = 0\n",
    "        mask[:, :, (kh // 2), (kw // 2 + 1):] = 0\n",
    "        # If mask type is A, then masking the center pixel\n",
    "        if mask_type == 'A':\n",
    "            mask[:, :, (kh // 2), (kw // 2)] = 0\n",
    "\n",
    "        # Making the mask the non-trainable parameter of the module\n",
    "        self.register_buffer('mask', mask.type(dtype=torch.bool))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.mask * self.weight, self.bias, \n",
    "                        self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e01823c-2bf5-44ee-ab1b-08abaf5192fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"A\" mask of the conv layer:\n",
      "[[[[ True  True  True  True  True]\n",
      "   [ True  True  True  True  True]\n",
      "   [ True  True False False False]\n",
      "   [False False False False False]\n",
      "   [False False False False False]]]]\n",
      "\n",
      "Type \"B\" mask of the conv layer:\n",
      "[[[[ True  True  True  True  True]\n",
      "   [ True  True  True  True  True]\n",
      "   [ True  True  True False False]\n",
      "   [False False False False False]\n",
      "   [False False False False False]]]]\n"
     ]
    }
   ],
   "source": [
    "# Check the mask in the masked conv layer\n",
    "print(\"Type \\\"A\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('A', 1, 1, 5).mask.numpy())\n",
    "\n",
    "print(\"\\nType \\\"B\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('B', 1, 1, 5).mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb8f94b-00a2-4378-97f0-89df0c676cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=in_channels, \n",
    "                                  out_channels=out_channels // 2,\n",
    "                                  kernel_size=1, padding='valid',\n",
    "                                  stride=1),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.pixel_conv = nn.Sequential(\n",
    "                        MaskedConv2D(\n",
    "                            mask_type='B',\n",
    "                            in_channels=out_channels // 2,\n",
    "                            out_channels=out_channels // 2,\n",
    "                            kernel_size=3,\n",
    "                            padding='same'),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=out_channels // 2,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=1,\n",
    "                                  padding='valid'),\n",
    "                        nn.ReLU())\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_x = self.conv_1(x)\n",
    "        conv_x = self.pixel_conv(conv_x)\n",
    "        conv_x = self.conv_2(conv_x)\n",
    "        return x + conv_x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6023b96-00df-4fa5-877b-abe629115406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResidualBlock(4, 4)\n",
    "# torchinfo.summary(model=model, input_size=(1, 4, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c982afc6-62e3-4688-94d2-d50c833edea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PixelCNN                                 [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-1                        [1, 128, 16, 16]          --\n",
       "│    └─MaskedConv2D: 2-1                 [1, 128, 16, 16]          6,400\n",
       "│    └─ReLU: 2-2                         [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-2                        [1, 128, 16, 16]          --\n",
       "│    └─ResidualBlock: 2-3                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-1              [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-2              [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-3              [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-4                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-4              [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-5              [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-6              [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-5                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-7              [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-8              [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-9              [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-6                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-10             [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-11             [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-12             [1, 128, 16, 16]          8,320\n",
       "│    └─ResidualBlock: 2-7                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-13             [1, 64, 16, 16]           8,256\n",
       "│    │    └─Sequential: 3-14             [1, 64, 16, 16]           36,928\n",
       "│    │    └─Sequential: 3-15             [1, 128, 16, 16]          8,320\n",
       "==========================================================================================\n",
       "Total params: 273,920\n",
       "Trainable params: 273,920\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 70.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 2.88\n",
       "Params size (MB): 1.10\n",
       "Estimated Total Size (MB): 3.98\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_filters, num_res_blocks):\n",
    "        super().__init__()\n",
    "        self.masked_conv_1 = nn.Sequential( \n",
    "                                MaskedConv2D(\n",
    "                                    mask_type='A',\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=num_filters,\n",
    "                                    kernel_size=7,\n",
    "                                    padding='same'),\n",
    "                                nn.ReLU()\n",
    "                             )\n",
    "\n",
    "        self.res_convs = nn.Sequential(*[\n",
    "                            ResidualBlock(\n",
    "                                in_channels=num_filters,\n",
    "                                out_channels=num_filters)\n",
    "                            for _ in range(num_res_blocks)])\n",
    "\n",
    "        # self.masked_conv_2\n",
    "        \n",
    "        # self.output_conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masked_conv_1(x)\n",
    "        x = self.res_convs(x)\n",
    "        return x\n",
    "\n",
    "model = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS)\n",
    "torchinfo.summary(model=model, input_size=(1, 1, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53892612-7fa7-4d15-815c-373a18087f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
