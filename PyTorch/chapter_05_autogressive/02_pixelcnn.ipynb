{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebf8f39-66c9-47bb-9ddc-5c2627f4a917",
   "metadata": {},
   "source": [
    "# PixelCNN for FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b06cd-02d6-4b0f-bff2-a1b1a05ced68",
   "metadata": {},
   "source": [
    "### In progress\n",
    "\n",
    "**The notebook has been adapted from the notebook provided in David Foster's Generative Deep Learning, 2nd Edition.**\n",
    "\n",
    "- Book: [Amazon](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184/ref=sr_1_1?keywords=generative+deep+learning%2C+2nd+edition&qid=1684708209&sprefix=generative+de%2Caps%2C93&sr=8-1)\n",
    "- Original notebook (tensorflow and keras): [Github](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393d1206-2d7f-421f-a92c-0a144102c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as Transforms\n",
    "import torchvision.utils as vuitls\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f66fa4-5289-4d7c-a5ca-3d95781ab430",
   "metadata": {},
   "source": [
    "## 0. Train Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d2939e-8ba4-46c0-8017-ca4e0a66ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "PIXEL_LEVELS = 12\n",
    "N_FILTERS = 128\n",
    "RESIDUAL_BLOCKS = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76640e9c-a3eb-4999-b7e5-b64cb0d8e048",
   "metadata": {},
   "source": [
    "## 1. Preparing FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "852fb189-d31f-4c39-8f01-628f00cfed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FashionData(Dataset):\n",
    "#     def __init__(self):\n",
    "#         transform_fn = Transforms.Compose([\n",
    "#                     Transforms.ToTensor(),\n",
    "#                     Transforms.Resize(IMAGE_SIZE, antialias=True),                        \n",
    "#                 ])\n",
    "        \n",
    "#         # Load FashionMNIST dataset\n",
    "#         fashion_ds = datasets.FashionMNIST('../../data', \n",
    "#                                           train=True, \n",
    "#                                           download=True,\n",
    "#                                           transform=transform_fn)\n",
    "        \n",
    "#         self.data = torch.stack([data[0] for data in fashion_ds])\n",
    "#         self.length = len(fashion_ds)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "#     def __getitem(self, idx):\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31a02a57-b75f-4ce6-894c-80697acc59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# ds = FashionData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d27d90e-5f5e-49c7-a768-7f453510359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn input image into (label-pixel representatin, pixel-wise labels)\n",
    "# def collate_fn(batch):\n",
    "#     batch = torch.stack([data[0] for data in batch])\n",
    "#     value_step = 1.0 / PIXEL_LEVELS\n",
    "#     labels = (batch / value_step).type(torch.long)\n",
    "#     imgs = labels.type(torch.float32) / PIXEL_LEVELS\n",
    "#     return imgs, labels\n",
    "\n",
    "def get_dataloader():\n",
    "    transform_fn = Transforms.Compose([\n",
    "                        Transforms.ToTensor(),\n",
    "                        Transforms.Resize(IMAGE_SIZE, antialias=True),                        \n",
    "                    ])\n",
    "    \n",
    "    # Load FashionMNIST dataset\n",
    "    fashion_ds = datasets.FashionMNIST('../../data', \n",
    "                                      train=True, \n",
    "                                      download=True,\n",
    "                                      transform=transform_fn)\n",
    "\n",
    "    # Get train dataloader\n",
    "    train_loader = DataLoader(fashion_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              # collate_fn=collate_fn)\n",
    "                             )\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee97742-720d-4c6b-bf5b-0c0870553f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check dataset\n",
    "def plot_imgs(batch, num_rows=2, num_cols=6):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for i in range(num_rows * num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "        ax.imshow(batch[i].permute(1, 2, 0), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "test_loader = get_dataloader()\n",
    "sample_data = next(iter(test_loader))\n",
    "\n",
    "plot_imgs(sample_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5f380-0074-4947-ba77-02c5a4bf6ae5",
   "metadata": {},
   "source": [
    "## 2. Build the PixelCNN\n",
    "\n",
    "This PyTorch implementation references pi-tau's GitHub repo: [Link](https://github.com/pi-tau/pixelcnn/blob/master/conv2d_mask.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1200c81a-8683-4290-a31a-ac4180f842da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building MaskedConv2D layer\n",
    "class MaskedConv2D(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, mask_type, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        kwargs['padding'] = 'same'\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "\n",
    "        assert mask_type in ['A', 'B'], 'Mask type should be either A or B'\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "        # Creating masks\n",
    "        kh, kw = kernel_size\n",
    "        mask = torch.ones_like(self.weight)\n",
    "        mask[:, :, (kh // 2 + 1):, :] = 0\n",
    "        mask[:, :, (kh // 2), (kw // 2 + 1):] = 0\n",
    "        # If mask type is A, then masking the center pixel\n",
    "        if mask_type == 'A':\n",
    "            mask[:, :, (kh // 2), (kw // 2)] = 0\n",
    "\n",
    "        # Making the mask the non-trainable parameter of the module\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2D, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e01823c-2bf5-44ee-ab1b-08abaf5192fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"A\" mask of the conv layer:\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1.]\n",
      "   [1. 1. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n",
      "\n",
      "Type \"B\" mask of the conv layer:\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 1. 1.]\n",
      "   [1. 1. 1. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Check the mask in the masked conv layer\n",
    "print(\"Type \\\"A\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('A', 1, 1, 5).mask.numpy())\n",
    "\n",
    "print(\"\\nType \\\"B\\\" mask of the conv layer:\")\n",
    "print(MaskedConv2D('B', 1, 1, 5).mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb8f94b-00a2-4378-97f0-89df0c676cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=in_channels, \n",
    "                                  out_channels=out_channels // 2,\n",
    "                                  kernel_size=1, padding='valid',\n",
    "                                  stride=1,\n",
    "                                  bias=False),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.pixel_conv = nn.Sequential(\n",
    "                        MaskedConv2D(\n",
    "                            mask_type='B',\n",
    "                            in_channels=out_channels // 2,\n",
    "                            out_channels=out_channels // 2,\n",
    "                            kernel_size=3,\n",
    "                            padding='same',\n",
    "                            bias=False),\n",
    "                        nn.ReLU())\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels=out_channels // 2,\n",
    "                                  out_channels=out_channels,\n",
    "                                  kernel_size=1,\n",
    "                                  padding='valid',\n",
    "                                  bias=False),\n",
    "                        nn.ReLU())\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_x = self.conv_1(x)\n",
    "        conv_x = self.pixel_conv(conv_x)\n",
    "        conv_x = self.conv_2(conv_x)\n",
    "        return conv_x + x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6023b96-00df-4fa5-877b-abe629115406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the residual block\n",
    "# model = ResidualBlock(4, 4)\n",
    "# torchinfo.summary(model=model, input_size=(1, 4, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c982afc6-62e3-4688-94d2-d50c833edea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PixelCNN                                 [1, 12, 16, 16]           --\n",
       "├─Sequential: 1-1                        [1, 128, 16, 16]          --\n",
       "│    └─MaskedConv2D: 2-1                 [1, 128, 16, 16]          6,400\n",
       "│    └─ReLU: 2-2                         [1, 128, 16, 16]          --\n",
       "├─Sequential: 1-2                        [1, 128, 16, 16]          --\n",
       "│    └─ResidualBlock: 2-3                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-1              [1, 64, 16, 16]           8,192\n",
       "│    │    └─Sequential: 3-2              [1, 64, 16, 16]           36,864\n",
       "│    │    └─Sequential: 3-3              [1, 128, 16, 16]          8,192\n",
       "│    └─ResidualBlock: 2-4                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-4              [1, 64, 16, 16]           8,192\n",
       "│    │    └─Sequential: 3-5              [1, 64, 16, 16]           36,864\n",
       "│    │    └─Sequential: 3-6              [1, 128, 16, 16]          8,192\n",
       "│    └─ResidualBlock: 2-5                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-7              [1, 64, 16, 16]           8,192\n",
       "│    │    └─Sequential: 3-8              [1, 64, 16, 16]           36,864\n",
       "│    │    └─Sequential: 3-9              [1, 128, 16, 16]          8,192\n",
       "│    └─ResidualBlock: 2-6                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-10             [1, 64, 16, 16]           8,192\n",
       "│    │    └─Sequential: 3-11             [1, 64, 16, 16]           36,864\n",
       "│    │    └─Sequential: 3-12             [1, 128, 16, 16]          8,192\n",
       "│    └─ResidualBlock: 2-7                [1, 128, 16, 16]          --\n",
       "│    │    └─Sequential: 3-13             [1, 64, 16, 16]           8,192\n",
       "│    │    └─Sequential: 3-14             [1, 64, 16, 16]           36,864\n",
       "│    │    └─Sequential: 3-15             [1, 128, 16, 16]          8,192\n",
       "├─Sequential: 1-3                        [1, 128, 16, 16]          --\n",
       "│    └─Sequential: 2-8                   [1, 128, 16, 16]          --\n",
       "│    │    └─MaskedConv2D: 3-16           [1, 128, 16, 16]          16,512\n",
       "│    │    └─ReLU: 3-17                   [1, 128, 16, 16]          --\n",
       "│    └─Sequential: 2-9                   [1, 128, 16, 16]          --\n",
       "│    │    └─MaskedConv2D: 3-18           [1, 128, 16, 16]          16,512\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 16, 16]          --\n",
       "├─Conv2d: 1-4                            [1, 12, 16, 16]           1,548\n",
       "==========================================================================================\n",
       "Total params: 307,212\n",
       "Trainable params: 307,212\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 78.65\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 3.43\n",
       "Params size (MB): 1.23\n",
       "Estimated Total Size (MB): 4.66\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_filters, num_res_blocks, ouput_size=PIXEL_LEVELS):\n",
    "        super().__init__()\n",
    "        self.masked_conv_1 = nn.Sequential( \n",
    "                                MaskedConv2D(\n",
    "                                    mask_type='A',\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=num_filters,\n",
    "                                    kernel_size=7,\n",
    "                                    padding='same'),\n",
    "                                nn.ReLU()\n",
    "                             )\n",
    "\n",
    "        self.res_convs = nn.Sequential(*[\n",
    "                            ResidualBlock(\n",
    "                                in_channels=num_filters,\n",
    "                                out_channels=num_filters)\n",
    "                            for _ in range(num_res_blocks)])\n",
    "\n",
    "        self.masked_conv_2 = nn.Sequential(*[\n",
    "                                nn.Sequential(\n",
    "                                    MaskedConv2D(\n",
    "                                        mask_type='B',\n",
    "                                        in_channels=num_filters,\n",
    "                                        out_channels=num_filters,\n",
    "                                        kernel_size=1,\n",
    "                                        padding='valid'),\n",
    "                                    nn.ReLU())\n",
    "                                for _ in range(2)],\n",
    "                            )\n",
    "\n",
    "        self.output_conv = nn.Conv2d(in_channels=num_filters,\n",
    "                                     out_channels=ouput_size,\n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding='valid')\n",
    "        # We don't need a softmax layer when using CrossEntropy Loss in PyTorch\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.masked_conv_1(x)\n",
    "        x = self.res_convs(x)\n",
    "        x = self.masked_conv_2(x)\n",
    "        x = self.output_conv(x)\n",
    "        return x\n",
    "\n",
    "model = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS)\n",
    "torchinfo.summary(model=model, input_size=(1, 1, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a103523-b6f8-4b1e-a1ba-43e65e03799a",
   "metadata": {},
   "source": [
    "## 3. Define the model, dataloader, objective, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e468fb7-901c-46e2-84d9-d521e899f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn = PixelCNN(N_FILTERS, RESIDUAL_BLOCKS).to(DEVICE)\n",
    "\n",
    "# if torch.__version__.split('.')[0] == '2':\n",
    "#     torch.set_float32_matmul_precision('high')\n",
    "#     # It is important to use eager backend here to avoid\n",
    "#     # distribution mismatch in training and predicting\n",
    "#     pixel_cnn = torch.compile(pixel_cnn, dynamic=True)\n",
    "#     print('model compiled')\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(pixel_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9093698-308f-46d3-b045-d41e0e2fde53",
   "metadata": {},
   "source": [
    "## 4. Traineer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7cc8d87-2773-46c8-9173-e74a0deeddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, dataloader, loss_fn, optim):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for imgs, _ in dataloader:\n",
    "        optim.zero_grad()\n",
    "        # imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits = model(imgs)\n",
    "        logits = logits.reshape(-1, PIXEL_LEVELS)\n",
    "        # labels = labels.reshape(-1)\n",
    "        target = Variable(imgs.data.reshape(-1) * PIXEL_LEVELS).long()\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c77c7a90-7220-4b6f-ae86-65bcc699775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4846654690659125\n",
      "2.484665539218927\n",
      "2.4846654934669608\n",
      "2.4846654807580815\n",
      "2.4846654843165674\n",
      "2.484665516342944\n",
      "2.4846655127844577\n",
      "2.4846654751661745\n",
      "2.484665546844255\n",
      "2.484665519393075\n",
      "2.484665499567223\n",
      "2.4846654583904533\n",
      "2.4846654934669608\n",
      "2.48466542077217\n",
      "2.484665437039536\n",
      "2.484665519393075\n",
      "2.4846654919418953\n",
      "2.4846654807580815\n",
      "2.4846655260016925\n",
      "2.4846655402356372\n",
      "2.4846654792330156\n",
      "2.4846655153262334\n",
      "2.4846654665241363\n",
      "2.4846655234599164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model, dataloader, loss_fn, optim)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, _ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/GDL_torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    loss = trainer(model, train_loader, loss_fn, optim)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb94f66-bcc2-4c3e-979c-68242026c4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
